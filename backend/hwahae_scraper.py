"""
í™”í•´(Hwahae) ì œí’ˆ ê²€ìƒ‰ ë° í¬ë¡¤ë§ ëª¨ë“ˆ - OpenAI ì¶”ì²œ ì œí’ˆ ì‹¤ì‹œê°„ ê²€ìƒ‰
"""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import quote, urljoin, parse_qs, urlparse
import re
from bs4 import BeautifulSoup
import time
import random
from playwright.async_api import async_playwright

logger = logging.getLogger(__name__)

class HwahaeProductScraper:
    """í™”í•´ ì œí’ˆ ì •ë³´ í¬ë¡¤ë§ í´ë˜ìŠ¤ - ì‹¤ì‹œê°„ ê²€ìƒ‰"""
    
    def __init__(self):
        self.base_url = "https://www.hwahae.co.kr"
        self.api_url = "https://www.hwahae.co.kr/api"
        self.session = None
        self.timeout = aiohttp.ClientTimeout(total=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.hwahae.co.kr/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
        }
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(headers=self.headers, timeout=self.timeout)
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def search_product_by_name(self, product_name: str, brand: str = None) -> Optional[Dict[str, Any]]:
        """Playwrightë¡œ í™”í•´ì—ì„œ ì‹¤ì œ ì œí’ˆ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„
            search_query = product_name
            if brand and brand.lower() not in product_name.lower():
                search_query = f"{brand} {product_name}"
            
            logger.info(f"ğŸ” Playwrightë¡œ í™”í•´ ê²€ìƒ‰: '{search_query}'")
            
            async with async_playwright() as p:
                # í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € ì‹¤í–‰
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                page = await context.new_page()
                
                try:
                    # í™”í•´ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                    search_url = f"https://www.hwahae.co.kr/search?keyword={quote(search_query)}&tab=goods"
                    logger.info(f"ê²€ìƒ‰ URL: {search_url}")
                    
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                    
                    # ì œí’ˆ ê²°ê³¼ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    await page.wait_for_timeout(2000)
                    
                    # í™”í•´ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ëŠ” ì œí’ˆ ìš”ì†Œ ì°¾ê¸°
                    await page.wait_for_timeout(3000)  # ë” ê¸´ ëŒ€ê¸° ì‹œê°„
                    
                    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì œí’ˆ ì°¾ê¸°
                    product_elements = []
                    
                    # ë°©ë²• 1: ì œí’ˆ ë§í¬ ì§ì ‘ ì°¾ê¸°
                    links = await page.query_selector_all('a[href*="/goods/"], a[href*="/products/"], a[href*="product"]')
                    if links:
                        product_elements = links
                        logger.info(f"ë°©ë²•1: {len(links)}ê°œ ì œí’ˆ ë§í¬ ë°œê²¬")
                    
                    # ë°©ë²• 2: ì œí’ˆ ì¹´ë“œë‚˜ ì•„ì´í…œ ì°¾ê¸°
                    if not product_elements:
                        cards = await page.query_selector_all('[class*="product"], [class*="item"], [class*="card"]')
                        if cards:
                            product_elements = cards
                            logger.info(f"ë°©ë²•2: {len(cards)}ê°œ ì œí’ˆ ì¹´ë“œ ë°œê²¬")
                    
                    # ë°©ë²• 3: ì´ë¯¸ì§€ê°€ ìˆëŠ” ìš”ì†Œ ì°¾ê¸° (ì œí’ˆì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
                    if not product_elements:
                        img_containers = await page.query_selector_all('div:has(img), article:has(img)')
                        if img_containers:
                            product_elements = img_containers
                            logger.info(f"ë°©ë²•3: {len(img_containers)}ê°œ ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ ë°œê²¬")
                    
                    if product_elements:
                        # ì²« ë²ˆì§¸ ì œí’ˆ ì •ë³´ ì¶”ì¶œ
                        first_product = product_elements[0]
                        
                        # í™”í•´ ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ì œí’ˆ ë§í¬ ì°¾ê¸°
                        product_url = ""
                        extracted_name = product_name
                        image_url = ""
                        
                        # í™”í•´ ì‚¬ì´íŠ¸ì˜ ì œí’ˆ ë§í¬ ì§ì ‘ ì°¾ê¸°
                        product_links = await page.query_selector_all('a[href*="goods/"]')
                        if not product_links:
                            product_links = await page.query_selector_all('a[href*="products/"]')
                        
                        logger.info(f"ì œí’ˆ ë§í¬ {len(product_links)}ê°œ ë°œê²¬")
                        
                        if product_links:
                            # ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ê´€ë ¨ìˆëŠ” ì œí’ˆ ì°¾ê¸°
                            best_match = None
                            best_score = 0
                            
                            for link in product_links[:5]:  # ì²˜ìŒ 5ê°œ ì œí’ˆë§Œ í™•ì¸
                                try:
                                    link_text = await link.inner_text()
                                    if not link_text:
                                        continue
                                    
                                    # ê²€ìƒ‰ì–´ì™€ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                                    search_words = search_query.lower().split()
                                    link_words = link_text.lower().split()
                                    
                                    score = 0
                                    for search_word in search_words:
                                        for link_word in link_words:
                                            if search_word in link_word or link_word in search_word:
                                                score += 1
                                    
                                    if score > best_score:
                                        best_score = score
                                        best_match = link
                                        
                                except Exception:
                                    continue
                            
                            # ë§¤ì¹­ëœ ì œí’ˆì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì œí’ˆ ì‚¬ìš©
                            selected_link = best_match if best_match else product_links[0]
                            
                            href = await selected_link.get_attribute('href')
                            if href:
                                # URLì—ì„œ ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
                                clean_href = href.split('?')[0]  # ? ì´í›„ íŒŒë¼ë¯¸í„° ì œê±°
                                product_url = f"https://www.hwahae.co.kr/{clean_href}" if not clean_href.startswith('http') else clean_href
                                logger.info(f"ì„ íƒëœ ì œí’ˆ URL: {product_url} (ë§¤ì¹­ì ìˆ˜: {best_score})")
                                
                                # ì œí’ˆëª…ì„ ë§í¬ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                                link_text = await selected_link.inner_text()
                                if link_text and len(link_text.strip()) > 2:
                                    extracted_name = link_text.strip()
                                else:
                                    # ë§í¬ ë‚´ë¶€ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸°
                                    name_elems = await selected_link.query_selector_all('span, div')
                                    for elem in name_elems:
                                        text = await elem.inner_text()
                                        if text and len(text.strip()) > 2:
                                            extracted_name = text.strip()
                                            break
                                
                                # ì´ë¯¸ì§€ë„ ë§í¬ ìš”ì†Œì—ì„œ ì¶”ì¶œ
                                img_elem = await selected_link.query_selector('img')
                                if img_elem:
                                    src = (await img_elem.get_attribute('src') or 
                                           await img_elem.get_attribute('data-src') or
                                           await img_elem.get_attribute('data-lazy-src'))
                                    if src and not src.startswith('data:'):
                                        image_url = f"https://www.hwahae.co.kr{src}" if src.startswith('/') else src
                                
                                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê°€ê²© ë¯¸ë¦¬ ì¶”ì¶œ ì‹œë„
                                try:
                                    price_in_search = await selected_link.query_selector('[class*="price"], [class*="cost"], span:has-text("ì›"), div:has-text("â‚©")')
                                    if price_in_search:
                                        price_text = await price_in_search.inner_text()
                                        if price_text and any(char.isdigit() for char in price_text):
                                            import re
                                            numbers = re.findall(r'\d+', price_text.replace(',', ''))
                                            if numbers:
                                                price_num = ''.join(numbers)
                                                if len(price_num) >= 3:
                                                    price = f"â‚©{int(price_num):,}"
                                                    logger.info(f"ğŸ’° ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê°€ê²© ì¶”ì¶œ: {price}")
                                except Exception:
                                    pass
                        
                        logger.info(f"ìµœì¢… URL: {product_url}")
                        logger.info(f"ìµœì¢… ì´ë¯¸ì§€: {image_url}")
                        
                        # ê¸°ë³¸ ê°€ê²© ì„¤ì •
                        price = "â‚©ê°€ê²© ë¬¸ì˜"
                        
                        # ê°€ê²© ì •ë³´ ì¶”ì¶œ (ì œí’ˆ í˜ì´ì§€ì—ì„œ)
                        if product_url:
                            try:
                                await page.goto(product_url, wait_until='domcontentloaded', timeout=10000)
                                await page.wait_for_timeout(1000)
                                
                                # ê°€ê²© ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‚¬ìš©)
                                price_selectors = [
                                    '[class*="price"]',
                                    '[class*="cost"]', 
                                    '[class*="amount"]',
                                    '[class*="won"]',
                                    'span:has-text("ì›")',
                                    'div:has-text("ì›")',
                                    '[data-testid*="price"]',
                                    '.price',
                                    '.cost',
                                    '*:has-text("â‚©")'
                                ]
                                
                                for selector in price_selectors:
                                    try:
                                        price_elem = await page.query_selector(selector)
                                        if price_elem:
                                            price_text = await price_elem.inner_text()
                                            if price_text and any(char.isdigit() for char in price_text):
                                                # ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ê°€ê²© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                                import re
                                                numbers = re.findall(r'\d+', price_text.replace(',', ''))
                                                if numbers:
                                                    price_num = ''.join(numbers)
                                                    if len(price_num) >= 3:  # ìµœì†Œ 3ìë¦¬ ì´ìƒ
                                                        price = f"â‚©{int(price_num):,}"
                                                        logger.info(f"ğŸ’° ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {price} (ì…€ë ‰í„°: {selector})")
                                                        break
                                    except Exception:
                                        continue
                                
                                # ì œí’ˆëª… ë‹¤ì‹œ í™•ì¸ (ìƒì„¸ í˜ì´ì§€ì—ì„œ ë” ì •í™•)
                                title_elem = await page.query_selector('h1, [class*="title"], [class*="name"]')
                                if title_elem:
                                    page_title = await title_elem.inner_text()
                                    if page_title:
                                        extracted_name = page_title
                            except Exception as e:
                                logger.warning(f"ì œí’ˆ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                        
                        # ì œí’ˆëª…ì—ì„œ ë³„ì ê³¼ ëŒ“ê¸€ìˆ˜ ì œê±°
                        cleaned_name = self._clean_product_name(extracted_name)
                        
                        logger.info(f"âœ… ì œí’ˆ ë°œê²¬: {cleaned_name}")
                        
                        return {
                            "product_name": cleaned_name.strip(),
                            "brand": brand or cleaned_name.split()[0],
                            "price": price,
                            "url": product_url,
                            "image_url": image_url or "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product",
                            "short_summary": f"{brand or 'ë¸Œëœë“œ'} {cleaned_name} ì œí’ˆì…ë‹ˆë‹¤."
                        }
                    
                    else:
                        logger.warning(f"'{search_query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        return None
                        
                except Exception as e:
                    logger.error(f"Playwright ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    return None
                finally:
                    await browser.close()
                    
        except Exception as e:
            logger.error(f"ì œí’ˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def _clean_product_name(self, product_name: str) -> str:
        """ì œí’ˆëª…ì—ì„œ ë³„ì , ëŒ“ê¸€ìˆ˜, ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°"""
        import re
        
        if not product_name:
            return product_name
        
        logger.info(f"ğŸ§¹ ì •ë¦¬ ì „ ì œí’ˆëª…: {product_name}")
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì œê±° (ì˜ˆ: "3 í† ë¦¬ë“ ë‹¤ì´ë¸Œì¸..." -> "í† ë¦¬ë“ ë‹¤ì´ë¸Œì¸...")
        product_name = re.sub(r'^\d+\s*', '', product_name)
        
        # ë³„ì  íŒ¨í„´ ì œê±° (ì˜ˆ: "4.66", "4.5ì ", "â˜…4.5" ë“±)
        product_name = re.sub(r'\d+\.\d+ì ?', '', product_name)
        product_name = re.sub(r'â˜…+\d+\.\d+', '', product_name)
        product_name = re.sub(r'â˜†+\d+\.\d+', '', product_name)
        
        # ëŒ“ê¸€ìˆ˜/ë¦¬ë·°ìˆ˜ íŒ¨í„´ ì œê±° (ì˜ˆ: "19,924", "20,333", "(1,234ê°œ)", "ë¦¬ë·° 123" ë“±)
        product_name = re.sub(r'\d{1,3}(,\d{3})+', '', product_name)
        product_name = re.sub(r'\d{4,}', '', product_name)  # 4ìë¦¬ ì´ìƒ ìˆ«ì ì œê±° (ëŒ“ê¸€ìˆ˜)
        product_name = re.sub(r'\(\d+ê°œ?\)', '', product_name)
        product_name = re.sub(r'ë¦¬ë·°\s*\d+', '', product_name)
        product_name = re.sub(r'í›„ê¸°\s*\d+', '', product_name)
        
        # ëì— ì˜¤ëŠ” ë³„ì ê³¼ ìˆ«ìë“¤ ì œê±° (ì˜ˆ: "í¬ë¦¼ 4 58" -> "í¬ë¦¼")
        product_name = re.sub(r'\s+\d+\s*$', '', product_name)  # ëì˜ ìˆ«ì
        product_name = re.sub(r'\s+\d+\s+\d+\s*$', '', product_name)  # ëì˜ ë‘ ìˆ«ì
        product_name = re.sub(r'\s+\d+\.\d+\s*$', '', product_name)  # ëì˜ ì†Œìˆ˜ì  ìˆ«ì
        
        # "ì œí’ˆì…ë‹ˆë‹¤" ê°™ì€ ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        product_name = re.sub(r'\s+ì œí’ˆì…ë‹ˆë‹¤\.?$', '', product_name)
        
        # ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        product_name = re.sub(r'\s+', ' ', product_name)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        product_name = product_name.strip()
        
        logger.info(f"ğŸ§¹ ì •ë¦¬ í›„ ì œí’ˆëª…: {product_name}")
        
        return product_name

    async def _search_in_database(self, product_name: str, brand: str = None) -> Optional[Dict[str, Any]]:
        """ë‚´ì¥ ë°ì´í„°ë² ì´ìŠ¤ ì™„ì „ ì œê±° - í•­ìƒ None ë°˜í™˜"""
        return None
    
    async def _extract_product_info(self, element, search_query: str) -> Dict[str, Any]:
        """HTML ìš”ì†Œì—ì„œ ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ìš”ì†Œ ì¶”ì¶œ ì‹œì‘: {element.name if hasattr(element, 'name') else 'Unknown'}")
            
            # ì œí’ˆ ë§í¬ ì¶”ì¶œ (í™”í•´ ì œí’ˆ í˜ì´ì§€ë§Œ)
            link_elem = element if element.name == 'a' else element.find('a')
            product_url = ""
            if link_elem and link_elem.get('href'):
                href = link_elem.get('href')
                if '/goods/' in href or '/products/' in href:
                    product_url = urljoin(self.base_url, href)
                    logger.info(f"ğŸ”— í™”í•´ ì œí’ˆ ë§í¬ ë°œê²¬: {product_url}")
                else:
                    logger.warning(f"í™”í•´ ì œí’ˆì´ ì•„ë‹Œ ë§í¬: {href}")
            
            # ì œí’ˆëª… ì¶”ì¶œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
            name_elem = (element.find(['h3', 'h4', 'span', 'div'], class_=re.compile(r'name|title|product', re.I)) or
                        element.find(string=re.compile(search_query, re.I)))
            
            if isinstance(name_elem, str):
                product_name = name_elem.strip()
            elif name_elem:
                product_name = name_elem.get_text(strip=True)
            else:
                product_name = search_query
            
            logger.info(f"ğŸ“ ì œí’ˆëª…: {product_name}")
            
            # ë¸Œëœë“œ ì¶”ì¶œ
            brand_elem = element.find(['span', 'div'], class_=re.compile(r'brand|company', re.I))
            brand = brand_elem.get_text(strip=True) if brand_elem else search_query.split()[0]
            
            # ê°€ê²© ì¶”ì¶œ
            price_elem = element.find(['span', 'div'], class_=re.compile(r'price|cost|won', re.I))
            price = "â‚©25,000"  # ê¸°ë³¸ê°’
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                if price_text and any(char.isdigit() for char in price_text):
                    price = price_text if price_text.startswith('â‚©') else f"â‚©{price_text}"
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì†ì„± ì‹œë„)
            img_elem = element.find('img')
            image_url = "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product"  # ê¸°ë³¸ê°’
            if img_elem:
                # ì—¬ëŸ¬ ì´ë¯¸ì§€ ì†ì„± ì‹œë„
                src = (img_elem.get('src') or 
                       img_elem.get('data-src') or 
                       img_elem.get('data-lazy-src') or
                       img_elem.get('data-original') or
                       img_elem.get('data-srcset'))
                
                if src and 'placeholder' not in src.lower():
                    # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if src.startswith('/'):
                        image_url = urljoin(self.base_url, src)
                    elif src.startswith('http'):
                        image_url = src
                    else:
                        image_url = urljoin(self.base_url, '/' + src)
                    
                    logger.info(f"ğŸ“· ì´ë¯¸ì§€ URL ì¶”ì¶œ: {image_url}")
            
            # í™”í•´ ì œí’ˆ í˜ì´ì§€ê°€ ìˆë‹¤ë©´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œë„
            if product_url and ('/goods/' in product_url or '/products/' in product_url):
                logger.info(f"ìƒì„¸ í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘ ì‹œë„: {product_url}")
                additional_info = await self._get_product_details(product_url)
                if additional_info and len(additional_info) > 3:
                    return additional_info
            
            return {
                "product_name": product_name,
                "brand": brand,
                "price": price,
                "url": product_url or f"{self.base_url}/search?q={quote(search_query)}",
                "image_url": image_url or "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product",
                "short_summary": f"{brand}ì˜ {product_name}ìœ¼ë¡œ í™”í•´ì—ì„œ ì¸ê¸° ìˆëŠ” ì œí’ˆì…ë‹ˆë‹¤."
            }
            
        except Exception as e:
            logger.error(f"ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {
                "product_name": search_query,
                "brand": "ë¸Œëœë“œëª…",
                "price": "â‚©25,000",
                "url": f"{self.base_url}/search?q={quote(search_query)}",
                "image_url": "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product",
                "short_summary": f"{search_query} ê´€ë ¨ ì œí’ˆì…ë‹ˆë‹¤."
            }
    
    async def _get_product_details(self, product_url: str) -> Optional[Dict[str, Any]]:
        """ì œí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            async with self.session.get(product_url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
                product_info = {}
                
                # ì œí’ˆëª…
                title_elem = soup.find(['h1', 'h2'], class_=re.compile(r'title|name|product'))
                if title_elem:
                    product_info['product_name'] = title_elem.get_text(strip=True)
                
                # ë¸Œëœë“œ
                brand_elem = soup.find(['span', 'div', 'a'], class_=re.compile(r'brand|company'))
                if brand_elem:
                    product_info['brand'] = brand_elem.get_text(strip=True)
                
                # ê°€ê²©
                price_elem = soup.find(['span', 'div'], class_=re.compile(r'price|cost'))
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    product_info['price'] = price_text if price_text.startswith('â‚©') else f"â‚©{price_text}"
                
                # ë©”ì¸ ì´ë¯¸ì§€
                img_elem = soup.find('img', class_=re.compile(r'main|product|detail'))
                if img_elem:
                    src = img_elem.get('src') or img_elem.get('data-src')
                    if src:
                        product_info['image_url'] = urljoin(self.base_url, src) if src.startswith('/') else src
                
                # ì œí’ˆ ì„¤ëª…
                desc_elem = soup.find(['div', 'p'], class_=re.compile(r'desc|summary|info'))
                if desc_elem:
                    product_info['short_summary'] = desc_elem.get_text(strip=True)[:100] + "..."
                
                product_info['url'] = product_url
                
                return product_info if len(product_info) > 2 else None
                
        except Exception as e:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None

    async def search_products_by_category(self, skin_type: str, skin_concerns: List[str], limit: int = 3) -> List[Dict[str, Any]]:
        """í”¼ë¶€ íƒ€ì…ê³¼ ê³ ë¯¼ì— ë”°ë¥¸ ì œí’ˆ ê²€ìƒ‰"""
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ë§¤í•‘
            category_mapping = {
                "dry": ["moisturizer", "cream", "serum"],
                "oily": ["cleanser", "toner", "serum"], 
                "sensitive": ["gentle", "calm", "sensitive"],
                "acne": ["acne", "pore", "bha"],
                "aging": ["anti-aging", "wrinkle", "firming"],
                "pigmentation": ["whitening", "vitamin-c", "spot"]
            }
            
            products = []
            
            # ë¯¸ë¦¬ ì •ì˜ëœ í™”í•´ ì¸ê¸° ì œí’ˆ ë°ì´í„°ë² ì´ìŠ¤
            hwahae_products = await self._get_popular_products_database()
            
            # í”¼ë¶€ íƒ€ì…ì— ë§ëŠ” ì œí’ˆ í•„í„°ë§
            for product in hwahae_products:
                if self._matches_skin_needs(product, skin_type, skin_concerns):
                    products.append(product)
                    if len(products) >= limit:
                        break
            
            # ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ì›¹ í¬ë¡¤ë§ ì‹œë„
            if len(products) < limit:
                additional_products = await self._crawl_category_products(skin_type, skin_concerns, limit - len(products))
                products.extend(additional_products)
            
            return products[:limit]
            
        except Exception as e:
            logger.error(f"í™”í•´ ì œí’ˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return await self._get_fallback_products()
    
    async def _get_popular_products_database(self) -> List[Dict[str, Any]]:
        """í™”í•´ ì¸ê¸° ì œí’ˆ ë°ì´í„°ë² ì´ìŠ¤ (ë¹„ì–´ìˆìŒ - í•˜ë“œì½”ë”© ì œê±°)"""
        return []
    
    def _matches_skin_needs(self, product: Dict, skin_type: str, skin_concerns: List[str]) -> bool:
        """ì œí’ˆì´ í”¼ë¶€ ë‹ˆì¦ˆì— ë§ëŠ”ì§€ í™•ì¸"""
        # í”¼ë¶€ íƒ€ì… ë§¤ì¹­
        if skin_type.lower() not in [t.lower() for t in product.get("skin_types", [])] and "all" not in product.get("skin_types", []):
            return False
        
        # í”¼ë¶€ ê³ ë¯¼ ë§¤ì¹­
        product_categories = [cat.lower() for cat in product.get("categories", [])]
        concern_keywords = {
            "acne": ["acne", "pore", "bha", "salicylic"],
            "dryness": ["hydrating", "moisturizer", "hyaluronic", "oil"],
            "sensitivity": ["sensitive", "gentle", "soothing", "calm"],
            "aging": ["anti-aging", "wrinkle", "firming", "peptide"],
            "pigmentation": ["whitening", "vitamin-c", "brightening", "spot"]
        }
        
        for concern in skin_concerns:
            if concern.lower() in concern_keywords:
                keywords = concern_keywords[concern.lower()]
                if any(keyword in product_categories for keyword in keywords):
                    return True
        
        return len(skin_concerns) == 0  # íŠ¹ë³„í•œ ê³ ë¯¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì œí’ˆ ê°€ëŠ¥
    
    async def _crawl_category_products(self, skin_type: str, skin_concerns: List[str], limit: int) -> List[Dict[str, Any]]:
        """ì‹¤ì œ í™”í•´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œí’ˆ í¬ë¡¤ë§ (ì œí•œì )"""
        try:
            # í™”í•´ì˜ ì¸ê¸° ì œí’ˆ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œë„
            url = f"{self.base_url}/ranking"
            
            if not self.session:
                return []
                
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    products = []
                    # ì œí’ˆ ìš”ì†Œ ì¶”ì¶œ (ì‹¤ì œ í™”í•´ HTML êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
                    product_elements = soup.find_all('div', class_='product-item')[:limit]
                    
                    for element in product_elements:
                        try:
                            product = {
                                "product_name": self._extract_text(element, '.product-name'),
                                "brand": self._extract_text(element, '.brand-name'),
                                "price": self._extract_text(element, '.price'),
                                "url": self._extract_link(element, 'a'),
                                "image_url": self._extract_image(element, 'img'),
                                "short_summary": "í™”í•´ì—ì„œ ì¸ê¸° ìˆëŠ” ì œí’ˆì…ë‹ˆë‹¤.",
                                "categories": ["popular"],
                                "skin_types": ["all"],
                                "rating": 4.0,
                                "review_count": 1000
                            }
                            products.append(product)
                        except Exception as e:
                            logger.warning(f"ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                    
                    return products
                    
        except Exception as e:
            logger.error(f"í™”í•´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
        return []
    
    def _extract_text(self, element, selector: str) -> str:
        """HTML ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            found = element.select_one(selector)
            return found.get_text(strip=True) if found else ""
        except:
            return ""
    
    def _extract_link(self, element, selector: str) -> str:
        """HTML ìš”ì†Œì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            found = element.select_one(selector)
            if found and found.get('href'):
                return urljoin(self.base_url, found.get('href'))
            return f"{self.base_url}/goods/product/default"
        except:
            return f"{self.base_url}/goods/product/default"
    
    def _extract_image(self, element, selector: str) -> str:
        """HTML ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        try:
            found = element.select_one(selector)
            if found and found.get('src'):
                return found.get('src')
            return "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product"
        except:
            return "https://via.placeholder.com/200x200/f0f0f0/999999?text=Product"
    
    async def _get_fallback_products(self) -> List[Dict[str, Any]]:
        """í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ - í•˜ë“œì½”ë”© ì™„ì „ ì œê±°"""
        return []


# OpenAI ì¶”ì²œ ì œí’ˆì„ í™”í•´ì—ì„œ ê²€ìƒ‰í•˜ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜
async def search_openai_products_on_hwahae(openai_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """OpenAIê°€ ì¶”ì²œí•œ ì œí’ˆë“¤ì„ í™”í•´ì—ì„œ ì‹¤ì œ ê²€ìƒ‰í•´ì„œ ì‹¤ì œ ì •ë³´ ìˆ˜ì§‘"""
    
    if not openai_products:
        logger.warning("OpenAI ì œí’ˆ ì¶”ì²œì´ ì—†ìŒ")
        return []
    
    real_products = []
    
    async with HwahaeProductScraper() as scraper:
        for i, ai_product in enumerate(openai_products):
            try:
                product_name = ai_product.get('product_name', '')
                brand = ai_product.get('brand', '')
                
                if not product_name:
                    logger.warning(f"ì œí’ˆ {i+1}: ì œí’ˆëª…ì´ ì—†ìŒ")
                    continue
                
                logger.info(f"ğŸ” ì œí’ˆ {i+1} í™”í•´ ê²€ìƒ‰: {brand} {product_name}")
                
                # í™”í•´ì—ì„œ ì‹¤ì œ ì œí’ˆ ê²€ìƒ‰
                real_product = await scraper.search_product_by_name(product_name, brand)
                
                if real_product:
                    # ì´ë¯¸ì§€ URLì´ í™”í•´ ì‚¬ì´íŠ¸ì¸ ê²½ìš° í”„ë¡ì‹œ ì²˜ë¦¬
                    image_url = real_product.get('image_url', 'https://via.placeholder.com/200x200/f0f0f0/999999?text=Product')
                    if 'hwahae.co.kr' in image_url or 'img.hwahae.co.kr' in image_url:
                        # í”„ë¡ì‹œë¥¼ í†µí•´ ì´ë¯¸ì§€ ì œê³µ
                        image_url = f"http://localhost:8000/proxy-image?url={quote(image_url)}"
                    
                    # OpenAIì˜ ì¶”ì²œ ì´ìœ ì™€ í™”í•´ì˜ ì‹¤ì œ ì •ë³´ ê²°í•©
                    combined_product = {
                        "reason": ai_product.get('reason', f"{product_name} ì œí’ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤."),
                        "product_name": real_product.get('product_name', product_name),
                        "brand": real_product.get('brand', brand),
                        "price": real_product.get('price', ai_product.get('price', 'â‚©25,000')),
                        "url": real_product.get('url', f"https://www.hwahae.co.kr/search?q={quote(product_name)}"),
                        "image_url": image_url,
                        "short_summary": real_product.get('short_summary', ai_product.get('short_summary', f'{brand} {product_name} ì œí’ˆì…ë‹ˆë‹¤.'))
                    }
                    real_products.append(combined_product)
                    logger.info(f"âœ… ì œí’ˆ {i+1} ê²€ìƒ‰ ì„±ê³µ: {real_product.get('product_name')}")
                else:
                    # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (í•˜ë“œì½”ë”© ì œê±°)
                    logger.warning(f"âŒ ì œí’ˆ {i+1} ê²€ìƒ‰ ì‹¤íŒ¨: {product_name} - ê±´ë„ˆë›°ê¸°")
                
                # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ (ë‹¨ì¶•)
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ì œí’ˆ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
    
    logger.info(f"ğŸ¯ ì´ {len(real_products)}/{len(openai_products)}ê°œ ì œí’ˆ ì²˜ë¦¬ ì™„ë£Œ")
    return real_products

# OpenAIë¥¼ ì‚¬ìš©í•œ í”¼ë¶€ ë¶„ì„ ê¸°ë°˜ ì œí’ˆ ì¶”ì²œ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)
async def get_hwahae_recommendations(skin_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """í”¼ë¶€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™”í•´ ì œí’ˆ ì¶”ì²œ"""
    
    try:
        # í”¼ë¶€ ë¶„ì„ì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
        skin_conditions = skin_analysis.get("skin_conditions", {})
        skin_type = "normal"  # ê¸°ë³¸ê°’
        concerns = []
        
        # í”¼ë¶€ íƒ€ì… ì¶”ì •
        if skin_conditions.get("dryness", {}).get("probability", 0) > 0.5:
            skin_type = "dry"
            concerns.append("dryness")
        elif skin_conditions.get("oiliness", {}).get("probability", 0) > 0.5:
            skin_type = "oily"
        
        # í”¼ë¶€ ê³ ë¯¼ ì¶”ì¶œ
        for condition, data in skin_conditions.items():
            if data.get("probability", 0) > 0.4:
                concerns.append(condition)
        
        # í™”í•´ ì œí’ˆ ê²€ìƒ‰
        async with HwahaeProductScraper() as scraper:
            products = await scraper.search_products_by_category(skin_type, concerns, limit=3)
        
        # í”„ë¡¬í”„íŠ¸ì— ë§ëŠ” í˜•íƒœë¡œ ë³€í™˜
        recommendations = []
        for product in products:
            recommendation = {
                "reason": f"ë¶„ì„ ê²°ê³¼ {skin_type} í”¼ë¶€ íƒ€ì…ì— {', '.join(concerns)} ê³ ë¯¼ì´ ìˆì–´ì„œ ì´ ì œí’ˆì´ ì í•©í•´.",
                "product_name": product["product_name"],
                "brand": product["brand"],
                "price": product["price"],
                "url": product["url"],
                "image_url": product["image_url"],
                "short_summary": product["short_summary"]
            }
            recommendations.append(recommendation)
        
        # ì „ì²´ ì‘ë‹µ êµ¬ì„±
        return {
            "expert_diagnosis": "í”¼ë¶€ ë¶„ì„ì„ í†µí•´ ë§ì¶¤í˜• ì œí’ˆì„ ì°¾ì•˜ì–´!",
            "predicted_skin_age": "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°ëœ í”¼ë¶€ë‚˜ì´ì•¼",
            "predicted_skin_sensitivity": "í˜„ì¬ í”¼ë¶€ ë¯¼ê°ë„ ìƒíƒœì•¼",
            "skin_tone_palette": ["ë¶„ì„ëœ í”¼ë¶€ í†¤ ì •ë³´"],
            "detailed_analysis": {
                "skin_condition": f"í˜„ì¬ {skin_type} íƒ€ì…ì˜ í”¼ë¶€ë¡œ ë³´ì´ê³ , {', '.join(concerns) if concerns else 'íŠ¹ë³„í•œ ë¬¸ì œëŠ” ì—†ì–´'} ìƒíƒœì•¼.",
                "key_points": "ë¶„ì„ëœ ì£¼ìš” í¬ì¸íŠ¸ë“¤ì´ì•¼.",
                "improvement_direction": "ì´ëŸ° ë°©í–¥ìœ¼ë¡œ ê´€ë¦¬í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„."
            },
            "purchase_considerations": [
                {
                    "reason": "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ë§¤ ê³ ë ¤ì‚¬í•­ì´ì•¼.",
                    "recommendation": "ì´ëŸ° ì ì„ ì°¸ê³ í•´ì„œ ì œí’ˆì„ ì„ íƒí•´ë´."
                }
            ],
            "product_recommendations": recommendations
        }
        
    except Exception as e:
        logger.error(f"í™”í•´ ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì œí’ˆ ë°˜í™˜
        async with HwahaeProductScraper() as scraper:
            fallback_products = await scraper._get_fallback_products()
            
        return {
            "expert_diagnosis": "í”¼ë¶€ê°€ ì „ë°˜ì ìœ¼ë¡œ ê±´ê°•í•œ ìƒíƒœì•¼!",
            "predicted_skin_age": "ì‹¤ì œ ë‚˜ì´ì™€ ë¹„ìŠ·í•´ ë³´ì—¬",
            "predicted_skin_sensitivity": "ë³´í†µ ìˆ˜ì¤€ì˜ ë¯¼ê°ë„ì•¼",
            "skin_tone_palette": ["ìì—°ìŠ¤ëŸ¬ìš´ í”¼ë¶€ í†¤ì´ì•¼"],
            "detailed_analysis": {
                "skin_condition": "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ í”¼ë¶€ ìƒíƒœë¥¼ ë³´ì´ê³  ìˆì–´.",
                "key_points": "ê¾¸ì¤€í•œ ê´€ë¦¬ë§Œ í•˜ë©´ ë¼.",
                "improvement_direction": "ê¸°ë³¸ì ì¸ ìŠ¤í‚¨ì¼€ì–´ ë£¨í‹´ì„ ìœ ì§€í•´ë´."
            },
            "purchase_considerations": [
                {
                    "reason": "ê¸°ë³¸ì ì¸ ê´€ë¦¬ ì œí’ˆì´ í•„ìš”í•´.",
                    "recommendation": "ìˆœí•˜ê³  ê²€ì¦ëœ ì œí’ˆì„ ì„ íƒí•´ë´."
                }
            ],
            "product_recommendations": [
                {
                    "reason": "ê²€ì¦ëœ ì¸ê¸° ì œí’ˆìœ¼ë¡œ ëª¨ë“  í”¼ë¶€íƒ€ì…ì— ì í•©í•´.",
                    "product_name": product["product_name"],
                    "brand": product["brand"],
                    "price": product["price"],
                    "url": product["url"],
                    "image_url": product["image_url"],
                    "short_summary": product["short_summary"]
                } for product in fallback_products
            ]
        }