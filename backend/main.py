# 2025ë…„ ìµœì‹  ë²„ì „ - AI í”¼ë¶€ ë¶„ì„ê¸° ë°±ì—”ë“œ (OpenAI API í†µí•©)
from dotenv import load_dotenv
load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

from fastapi import FastAPI, File, UploadFile, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import cv2
import numpy as np
from PIL import Image
import io
import base64
from typing import Dict, List, Optional, Any, Tuple, Union
import logging
from dataclasses import dataclass, asdict
import math
import requests
import asyncio
from scipy import ndimage
from sklearn.cluster import KMeans
import time
import os
import uuid
import aiofiles
import aiohttp
from pathlib import Path
from hwahae_scraper import search_openai_products_on_hwahae
from transformers import ViTFeatureExtractor, ViTForImageClassification
import torch
from ultralytics import YOLO
from openai import OpenAI
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
TEMP_IMAGES_DIR = Path("temp_images")
TEMP_IMAGES_DIR.mkdir(exist_ok=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (API í‚¤ê°€ ì—†ì–´ë„ ì„œë²„ ì‹œì‘ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •)
openai_client = None
try:
    api_key = os.environ.get("OPENAI_API_KEY")
    if api_key:
        openai_client = OpenAI(
            api_key=api_key,
            timeout=60.0  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ê³ ë ¤)
        )
        logger.info("âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ! (60ì´ˆ íƒ€ì„ì•„ì›ƒ)")
    else:
        logger.warning("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OpenAI ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
except Exception as e:
    logger.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_client = None

# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì„ì‹œ ì €ì¥ í•¨ìˆ˜
async def download_and_save_image(image_url: str, session_id: str) -> str:
    """í™”í•´ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì„œ ì„ì‹œ ì €ì¥í•˜ê³  ë¡œì»¬ URL ë°˜í™˜"""
    try:
        if not image_url or 'placeholder' in image_url:
            return image_url
        
        # ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        file_extension = '.jpg'
        if '.png' in image_url:
            file_extension = '.png'
        elif '.webp' in image_url:
            file_extension = '.webp'
        
        filename = f"{session_id}_{uuid.uuid4().hex[:8]}{file_extension}"
        filepath = TEMP_IMAGES_DIR / filename
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        async with aiohttp.ClientSession() as session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Referer': 'https://www.hwahae.co.kr/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
            }
            
            async with session.get(image_url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    async with aiofiles.open(filepath, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            await f.write(chunk)
                    
                    logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {filename}")
                    return f"/temp-image/{filename}"
                else:
                    logger.warning(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                    return image_url
    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return image_url

async def cleanup_session_images(session_id: str):
    """ì„¸ì…˜ IDë¡œ ì‹œì‘í•˜ëŠ” ì„ì‹œ ì´ë¯¸ì§€ë“¤ ì‚­ì œ"""
    try:
        for filepath in TEMP_IMAGES_DIR.glob(f"{session_id}_*"):
            filepath.unlink()
            logger.info(f"ğŸ—‘ï¸ ì„ì‹œ ì´ë¯¸ì§€ ì‚­ì œ: {filepath.name}")
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì •ë¦¬ ì˜¤ë¥˜: {e}")

async def process_product_images(products: list, session_id: str) -> list:
    """ì œí’ˆ ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ URLë¡œ ë³€ê²½"""
    processed_products = []
    
    for product in products:
        processed_product = product.copy()
        image_url = product.get('image_url', '')
        
        if image_url and 'hwahae.co.kr' in image_url:
            # í™”í•´ ì´ë¯¸ì§€ì¸ ê²½ìš° ë‹¤ìš´ë¡œë“œ
            local_url = await download_and_save_image(image_url, session_id)
            processed_product['image_url'] = local_url
            processed_product['original_image_url'] = image_url  # ì›ë³¸ URL ë³´ì¡´
        
        processed_products.append(processed_product)
    
    return processed_products

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ì‹œ ì„¤ì •ê³¼ ì¢…ë£Œì‹œ ì •ë¦¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¼ì´í”„ìŠ¤íŒ¬ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    global analyzer
    logger.info("ğŸš€ 2025ë…„ ìµœì‹  AI í”¼ë¶€ ë¶„ì„ê¸° ì„œë²„ ì‹œì‘...")
    analyzer = ModernSkinAnalyzer()
    logger.info("âœ… 2025ë…„ AI ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    yield
    if analyzer:
        await analyzer.close_session()

app = FastAPI(
    title="AI í”¼ë¶€ ë¶„ì„ê¸° API", 
    version="3.0.0",
    description="2025ë…„ ìµœì‹  ê¸°ìˆ  ê¸°ë°˜ í´ë¼ìš°ë“œ í”¼ë¶€ ë¶„ì„ ì„œë¹„ìŠ¤ (OpenAI API í†µí•©)",
    lifespan=lifespan
)

# CORS ì„¤ì • (2025ë…„ ë³´ì•ˆ ê°•í™”)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

@dataclass
class SkinAnalysisResult:
    skin_type: str
    moisture_level: int
    oil_level: int
    blemish_count: int
    skin_tone: str
    wrinkle_level: int
    pore_size: str
    overall_score: int
    avg_skin_color: Dict[str, float]
    face_detected: bool
    confidence: float
    skin_area_percentage: float
    detected_features: List[str]
    processing_time: float
    api_method: str
    analysis_version: str = "2025.1.0"
    age_range: str = "ë¶„ì„ ë¶ˆê°€"
    age_confidence: float = 0.0
    acne_lesions: List[Dict] = None
    image_size: Dict[str, int] = None
    # OpenAI API ê²°ê³¼ ì¶”ê°€
    ai_analysis: Dict = None
    ai_recommendations: List[Dict] = None
    personalized_tips: List[str] = None

class ModernSkinAnalyzer:
    def __init__(self):
        # 2025ë…„ ìµœì‹  Hugging Face API ì—”ë“œí¬ì¸íŠ¸
        self.hf_api_base = "https://api-inference.huggingface.co/models"
        
        # ìµœì‹  AI ëª¨ë¸ë“¤ (2025ë…„)
        self.models = {
            "face_parsing": "jonathandinu/face-parsing",
            "face_detection": "ultralytics/yolov8n-face",
            "age_analysis": "nateraw/vit-age-classifier",
            "skin_analysis": "microsoft/DialoGPT-medium"
        }
        
        self.session = None
        
        # OpenCV ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
        try:
            cascade_path = os.path.join(os.path.dirname(cv2.__file__), 'data', 'haarcascade_frontalface_default.xml')
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if self.face_cascade.empty():
                raise ValueError("ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.info("âœ¨ OpenCV Face Detection ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            logger.error(f"âŒ OpenCV ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise e
            
        # ë‚˜ì´ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”
        self.age_model, self.age_transforms = self.init_age_model()
        
        # YOLOv8 ì—¬ë“œë¦„ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™”
        try:
            model_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'model', 'yolov8m-acnedetect-best.pt')
            if os.path.exists(model_path):
                self.acne_model = YOLO(model_path)
                logger.info("âœ¨ YOLOv8 ì—¬ë“œë¦„ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            else:
                logger.warning(f"âš ï¸ YOLOv8 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                self.acne_model = None
        except Exception as e:
            logger.error(f"âŒ YOLOv8 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.acne_model = None
        
        self.min_face_confidence = 0.3  # ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì–¼êµ´ ê°ì§€ í–¥ìƒ
        logger.info("ğŸš€ 2025ë…„ ìµœì‹  AI í”¼ë¶€ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def detect_faces(self, image: np.ndarray) -> List[Dict]:
        """OpenCVë¥¼ ì‚¬ìš©í•œ ì–¼êµ´ ê°ì§€"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ì–¼êµ´ ê°ì§€ (ì ì ˆí•œ ë¯¼ê°ë„)
            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,   # ì•ˆì •ì ì¸ ê°ì§€
                minNeighbors=5,    # ì ì ˆí•œ ë¯¼ê°ë„
                minSize=(30, 30),  # ìµœì†Œ ì–¼êµ´ í¬ê¸°
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            face_list = []
            for (x, y, w, h) in faces:
                face_list.append({
                    "x": int(x),
                    "y": int(y),
                    "width": int(w),
                    "height": int(h),
                    "confidence": 0.8  # OpenCVëŠ” ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©
                })
            
            return face_list
            
        except Exception as e:
            logger.error(f"âŒ ì–¼êµ´ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
            return []

    async def call_openai_skin_analysis(self, cv_results: Dict[str, Any], chatbot_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """OpenAI Responses APIë¥¼ ì‚¬ìš©í•œ ì‚¬ìš©ì ì¹œí™”ì  í”¼ë¶€ë¶„ì„"""
        global openai_client
        
        if not openai_client:
            return {
                "success": False,
                "error": "OpenAI API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "message": "OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            }
        
        try:
            # CV ëª¨ë¸ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜ (ìƒì„¸ ë°ì´í„° ì¶”ê°€)
            cv_analysis = {
                "ê¸°ë³¸ì •ë³´": {
                    "í”¼ë¶€íƒ€ì…": cv_results.get("skin_type", "ì •ìƒ"),
                    "ìˆ˜ë¶„ë„": cv_results.get("moisture_level", 50),
                    "ìœ ë¶„ë„": cv_results.get("oil_level", 50),
                    "í”¼ë¶€í†¤": cv_results.get("skin_tone", "ì¤‘ê°„í†¤"),
                    "ë‚˜ì´ëŒ€": cv_results.get("age_range", "20-30"),
                    "ì¢…í•©ì ìˆ˜": cv_results.get("overall_score", 70),
                    "ì‹ ë¢°ë„": round(cv_results.get("confidence", 0.8) * 100, 1)
                },
                "í”¼ë¶€ìƒíƒœ": {
                    "ì—¬ë“œë¦„ê°ì§€ê°œìˆ˜": len(cv_results.get("acne_lesions", [])),
                    "ì¡í‹°ê°œìˆ˜": cv_results.get("blemish_count", 0),
                    "ì£¼ë¦„ì •ë„": cv_results.get("wrinkle_level", 1),
                    "ëª¨ê³µí¬ê¸°": cv_results.get("pore_size", "ë³´í†µ"),
                    "í”¼ë¶€ë°ê¸°": round(cv_results.get("avg_skin_color", {}).get("r", 150), 1),
                    "í”¼ë¶€ê· ì¼ë„": round(cv_results.get("skin_uniformity", 0.7) * 100, 1) if "skin_uniformity" in str(cv_results) else 70,
                    "í”¼ë¶€ê±´ê°•ì ìˆ˜": round(cv_results.get("skin_health_score", 75), 1) if "skin_health_score" in str(cv_results) else 75
                },
                "ìƒì„¸ë¶„ì„": {
                    "í‰ê· í”¼ë¶€ìƒ‰ìƒ": cv_results.get("avg_skin_color", {"r": 150, "g": 130, "b": 110}),
                    "í”¼ë¶€í…ìŠ¤ì²˜ë³€í™”ë„": round(cv_results.get("skin_texture_variance", 100), 1) if "skin_texture_variance" in str(cv_results) else 100,
                    "ì—¬ë“œë¦„ìœ„ì¹˜ì •ë³´": [
                        {
                            "x": lesion.get("x", 0),
                            "y": lesion.get("y", 0),
                            "í¬ê¸°": f"{lesion.get('width', 5)}x{lesion.get('height', 5)}",
                            "ì‹ ë¢°ë„": round(lesion.get("confidence", 0.8) * 100, 1)
                        } for lesion in cv_results.get("acne_lesions", [])[:5]  # ìµœëŒ€ 5ê°œë§Œ
                    ]
                }
            }

            # ì±—ë´‡ ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° ì¶”ê°€
            chatbot_info = ""
            if chatbot_data:
                chatbot_info = f"\n\nğŸ“‹ ì‚¬ìš©ì ì„¤ë¬¸ì¡°ì‚¬ ì •ë³´:\n{json.dumps(chatbot_data, ensure_ascii=False, indent=2)}"

            # í”Œë ˆì´ê·¸ë¼ìš´ë“œ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•  ë°ì´í„°ë§Œ êµ¬ì„± - JSON ì¶œë ¥ ê°•ì œ
            input_data = f"""ë¶„ì„ ë°ì´í„°: {json.dumps(cv_analysis, ensure_ascii=False, indent=2)}{chatbot_info}

IMPORTANT: ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì„¤ëª…ë¬¸ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì²« ê¸€ìëŠ” {{ë¡œ ì‹œì‘í•˜ê³  ë§ˆì§€ë§‰ ê¸€ìëŠ” }}ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤."""

            # Responses API ì‚¬ìš© (í”„ë¡¬í”„íŠ¸ ID ê¸°ë°˜) - ë”¥ ë¦¬ì„œì¹˜ ëª¨ë¸ìš© ë„êµ¬ ì¶”ê°€
            response = openai_client.responses.create(
                prompt={
                    "id": "pmpt_68775616297c8194b9f6cc7b3dc04dfa0084b202623da423",
                    "version": "18"
                },
                input=[
                    {
                        "role": "user",
                        "content": input_data
                    }
                ],
                tools=[
                    {
                        "type": "web_search_preview"
                    }
                ]
            )

            # ì‘ë‹µ íŒŒì‹± (Responses API ë°©ì‹)
            logger.info(f"OpenAI Response íƒ€ì…: {type(response)}")
            
            # ìƒˆë¡œìš´ API ë°©ì‹ì—ì„œ ì‘ë‹µ ì¶”ì¶œ
            try:
                ai_response = ""
                
                # Response ê°ì²´ì—ì„œ output ì¶”ì¶œ
                if hasattr(response, 'output') and response.output:
                    logger.info(f"Output ì•„ì´í…œ ìˆ˜: {len(response.output)}")
                    for i, output_item in enumerate(response.output):
                        logger.info(f"Output ì•„ì´í…œ {i} íƒ€ì…: {type(output_item)}")
                        
                        # ResponseOutputMessage íƒ€ì…ì¸ì§€ í™•ì¸
                        if hasattr(output_item, 'content') and output_item.content:
                            logger.info(f"Content ì•„ì´í…œ ìˆ˜: {len(output_item.content)}")
                            for j, content_item in enumerate(output_item.content):
                                logger.info(f"Content ì•„ì´í…œ {j} íƒ€ì…: {type(content_item)}")
                                
                                # ResponseOutputTextì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                if hasattr(content_item, 'text'):
                                    try:
                                        ai_response = content_item.text
                                        logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {ai_response[:100]}...")
                                        break
                                    except Exception as text_error:
                                        logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {text_error}")
                                        continue
                            if ai_response:
                                break
                        
                        # WebSearch ê²°ê³¼ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                        elif hasattr(output_item, 'type') and 'web_search' in str(output_item.type):
                            logger.info(f"ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê±´ë„ˆë›°ê¸°: {output_item.type}")
                            continue
                
                # ì‘ë‹µì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ ì‘ë‹µ ë¡œê¹…
                if not ai_response:
                    logger.error("ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    logger.error(f"ì „ì²´ Response ê°ì²´: {response}")
                    ai_response = "OpenAI API ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        
            except Exception as e:
                logger.error(f"ì‘ë‹µ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                logger.error(f"Response ê°ì²´ íƒ€ì…: {type(response)}")
                ai_response = f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
                
            logger.info(f"OpenAI í”„ë¡¬í”„íŠ¸ ì‘ë‹µ (ì²˜ìŒ 200ì): {ai_response[:200]}...")
            
            # JSON íŒŒì‹± ì‹œë„ - ê°œì„ ëœ ë¡œì§
            try:
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±° ì‹œë„
                cleaned_response = ai_response.strip()
                
                # JSONì´ í…ìŠ¤íŠ¸ ì¤‘ê°„ì— ìˆëŠ” ê²½ìš° ì¶”ì¶œ
                json_start = cleaned_response.find('{')
                json_end = cleaned_response.rfind('}')
                
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    json_part = cleaned_response[json_start:json_end+1]
                    logger.info(f"JSON ë¶€ë¶„ ì¶”ì¶œë¨: {json_part[:200]}...")
                    ai_analysis = json.loads(json_part)
                else:
                    # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                    if cleaned_response.startswith('```json'):
                        cleaned_response = cleaned_response[7:]
                    if cleaned_response.startswith('```'):
                        cleaned_response = cleaned_response[3:]
                    if cleaned_response.endswith('```'):
                        cleaned_response = cleaned_response[:-3]
                        
                    cleaned_response = cleaned_response.strip()
                    ai_analysis = json.loads(cleaned_response)
                
                # í”Œë ˆì´ê·¸ë¼ìš´ë“œ JSON êµ¬ì¡°ì— ë§ì¶° ì‘ë‹µ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
                if not ai_analysis.get("expert_diagnosis"):
                    ai_analysis["expert_diagnosis"] = "AI ë¶„ì„ì„ í†µí•´ í”¼ë¶€ ìƒíƒœë¥¼ í™•ì¸í–ˆì–´, ë§ì¶¤ ê´€ë¦¬ë¡œ ë” ê±´ê°•í•´ì§ˆ ìˆ˜ ìˆì„ ê±°ì•¼!"
                
                if not ai_analysis.get("detailed_analysis"):
                    ai_analysis["detailed_analysis"] = {
                        "skin_condition": "í˜„ì¬ í”¼ë¶€ ìƒíƒœëŠ” ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•´ ë³´ì—¬. ê¸°ë³¸ì ì¸ ê´€ë¦¬ë§Œ ì˜í•´ë„ ì¶©ë¶„í•  ê²ƒ ê°™ì•„.",
                        "key_points": "íŠ¹ë³„í•œ ë¬¸ì œì ì€ ì—†ì§€ë§Œ ê¾¸ì¤€í•œ ë³´ìŠµì´ ì¤‘ìš”í•´.",
                        "improvement_direction": "ë§¤ì¼ í´ë Œì§•ê³¼ ë³´ìŠµì„ ì±™ê¸°ê³ , ìì™¸ì„  ì°¨ë‹¨ì„ ìŠì§€ ë§ì."
                    }
                
                # OpenAI ì‘ë‹µì— ê¸°ë³¸ê°’ ì„¤ì • (ì œí’ˆ ì¶”ì²œì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
                if not ai_analysis.get("product_recommendations"):
                    logger.warning("OpenAI ì‘ë‹µì— product_recommendationsê°€ ì—†ìŒ")
                    ai_analysis["product_recommendations"] = []
                else:
                    # ì œí’ˆ ì¶”ì²œ ë°ì´í„° ìƒì„¸ ë¡œê¹…
                    products = ai_analysis.get("product_recommendations", [])
                    logger.info(f"âœ… OpenAI ì œí’ˆ ì¶”ì²œ ê°œìˆ˜: {len(products)}")
                    for i, product in enumerate(products):
                        logger.info(f"ì œí’ˆ {i+1}: {product.get('product_name', 'Unknown')}")
                        logger.info(f"  ë¸Œëœë“œ: {product.get('brand', 'Unknown')}")
                        logger.info(f"  ê°€ê²©: {product.get('price', 'Unknown')}")
                        logger.info(f"  URL: {product.get('url', 'Unknown')}")
                        logger.info(f"  ì´ë¯¸ì§€ URL: {product.get('image_url', 'Unknown')}")
                        logger.info(f"  ìš”ì•½: {product.get('short_summary', 'Unknown')}")
                    
                    # OpenAI ì¶”ì²œ ì œí’ˆì„ í™”í•´ì—ì„œ ì‹¤ì œ ê²€ìƒ‰ (ê°•ì œ ì‹¤í–‰)
                    logger.info(f"ğŸš€ ê°•ì œë¡œ í™”í•´ ê²€ìƒ‰ ì‹¤í–‰ - OpenAI ì œí’ˆ ìˆ˜: {len(products)}")
                    if True:  # í•­ìƒ ì‹¤í–‰
                        try:
                            logger.info("ğŸ” OpenAI ì¶”ì²œ ì œí’ˆì„ í™”í•´ì—ì„œ ê²€ìƒ‰ ì‹œì‘...")
                            
                            # í™”í•´ ê²€ìƒ‰ì„ 30ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì œí•œ
                            real_products = await asyncio.wait_for(
                                search_openai_products_on_hwahae(products), 
                                timeout=30.0
                            )
                            
                            if real_products:
                                # ì„¸ì…˜ ID ìƒì„± (ë¶„ì„ ìš”ì²­ë³„ ê³ ìœ  ID)
                                session_id = str(uuid.uuid4())[:8]
                                
                                # ì œí’ˆ ì´ë¯¸ì§€ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ URLë¡œ ë³€ê²½
                                processed_products = await process_product_images(real_products, session_id)
                                
                                # ì‹¤ì œ í™”í•´ ì œí’ˆ ì •ë³´ë¡œ êµì²´
                                logger.info(f"ğŸ”„ OpenAI ì›ë³¸ ì œí’ˆ ìˆ˜: {len(ai_analysis.get('product_recommendations', []))}")
                                logger.info(f"ğŸ”„ í™”í•´ ê²€ìƒ‰ ê²°ê³¼ ì œí’ˆ ìˆ˜: {len(processed_products)}")
                                
                                for i, product in enumerate(processed_products):
                                    logger.info(f"ğŸ” í™”í•´ ì œí’ˆ {i+1}: {product.get('product_name')} | URL: {product.get('url')} | ì´ë¯¸ì§€: {product.get('image_url')}")
                                
                                ai_analysis["product_recommendations"] = processed_products
                                ai_analysis["session_id"] = session_id
                                logger.info(f"âœ… í™”í•´ ê²€ìƒ‰ ë° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(processed_products)}ê°œ ì œí’ˆ")
                            else:
                                logger.warning("âŒ í™”í•´ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•¨, OpenAI ì •ë³´ ìœ ì§€")
                        except asyncio.TimeoutError:
                            logger.warning("â° í™”í•´ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ(30ì´ˆ), OpenAI ì›ë³¸ ì •ë³´ ìœ ì§€")
                        except Exception as search_error:
                            logger.error(f"í™”í•´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {search_error}")
                            logger.info("OpenAI ì›ë³¸ ì œí’ˆ ì •ë³´ ìœ ì§€")
                
                return {
                    "success": True,
                    "analysis": ai_analysis,
                    "raw_response": ai_response
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ì •ì œëœ ì‘ë‹µ: {cleaned_response[:300]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ì œê³µ
                logger.error("OpenAI ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì‘ë‹µ ì œê³µ")
                return {
                    "success": False,
                    "error": f"OpenAI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                    "raw_response": ai_response,
                    "note": "OpenAIê°€ ì˜¬ë°”ë¥¸ JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŒ"
                }

        except asyncio.TimeoutError:
            logger.error("OpenAI API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
            return {
                "success": False,
                "error": "OpenAI API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ)"
            }
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            }
    
    async def init_session(self):
        """ë¹„ë™ê¸° HTTP ì„¸ì…˜ ì´ˆê¸°í™” (2025ë…„ ì„±ëŠ¥ ìµœì í™”)"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=30)
            connector = aiohttp.TCPConnector(limit=20, ttl_dns_cache=300)
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={"User-Agent": "SkinAnalyzer-2025/3.0"}
            )
    
    async def close_session(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    def preprocess_image_2025(self, image: np.ndarray) -> np.ndarray:
        """2025ë…„ í–¥ìƒëœ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
            
        # 2025ë…„ ìµœì í™”: ë™ì  í¬ê¸° ì¡°ì •
        height, width = image_rgb.shape[:2]
        
        # AI ëª¨ë¸ì— ìµœì í™”ëœ í¬ê¸° (2025ë…„ í‘œì¤€)
        target_size = 640 if max(height, width) > 1080 else 512
        
        if max(height, width) > target_size:
            scale = target_size / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            # 2025ë…„ ìµœì‹  ë³´ê°„ë²• ì‚¬ìš©
            image_rgb = cv2.resize(image_rgb, (new_width, new_height), 
                                 interpolation=cv2.INTER_LANCZOS4)
        
        # 2025ë…„ ì¶”ê°€: ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
        image_rgb = cv2.bilateralFilter(image_rgb, 9, 75, 75)
            
        return image_rgb
    
    def image_to_bytes(self, image: np.ndarray, quality: int = 90) -> bytes:
        """ìµœì í™”ëœ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë³€í™˜ (2025ë…„)"""
        pil_image = Image.fromarray(image)
        img_byte_arr = io.BytesIO()
        
        # 2025ë…„ ìµœì í™”: ì ì‘í˜• í’ˆì§ˆ
        if image.shape[0] * image.shape[1] > 500000:  # ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€
            quality = 85
        
        pil_image.save(img_byte_arr, format='JPEG', quality=quality, optimize=True)
        return img_byte_arr.getvalue()
    
    async def call_hf_api_2025(self, model_name: str, image_bytes: bytes) -> Dict:
        """2025ë…„ ìµœì‹  Hugging Face API í˜¸ì¶œ"""
        await self.init_session()
        
        url = f"{self.hf_api_base}/{self.models[model_name]}"
        headers = {
            "Content-Type": "application/octet-stream",
            "X-Request-ID": f"skin-analyzer-{int(time.time())}",
        }
        
        try:
            async with self.session.post(url, headers=headers, data=image_bytes) as response:
                if response.status == 200:
                    result = await response.json()
                    return {"success": True, "data": result}
                elif response.status == 503:
                    return {
                        "success": False, 
                        "error": "model_loading", 
                        "message": "AI ëª¨ë¸ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. 30ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
                    }
                else:
                    error_text = await response.text()
                    return {
                        "success": False, 
                        "error": "api_error", 
                        "message": f"API ì˜¤ë¥˜: {error_text[:100]}"
                    }
                    
        except asyncio.TimeoutError:
            return {
                "success": False, 
                "error": "timeout", 
                "message": "AI ë¶„ì„ ì‹œê°„ ì´ˆê³¼. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }
        except Exception as e:
            logger.error(f"HF API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return {
                "success": False, 
                "error": "network_error", 
                "message": f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
            }
    
    def detect_face(self, image: np.ndarray) -> Dict:
        """OpenCVë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì–¼êµ´ ê°ì§€"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ì–¼êµ´ ê°ì§€ (ì ë‹¹í•œ ê¸°ì¤€ìœ¼ë¡œ ë³µì›)
            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,    # ì•ˆì •ì ì¸ ìŠ¤ì¼€ì¼ë§
                minNeighbors=4,     # ì ë‹¹í•œ ê¸°ì¤€ (8 -> 4)
                minSize=(50, 50),   # ì ë‹¹í•œ ìµœì†Œ í¬ê¸° (80 -> 50)
                maxSize=(500, 500), # ë” í° ìµœëŒ€ í¬ê¸°
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            if len(faces) == 0:
                return {
                    "face_detected": False,
                    "confidence": 0.0,
                    "bbox": None
                }
            
            # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ (ì¤‘ì•™ì— ìˆëŠ” ì–¼êµ´ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
            best_face = max(faces, key=lambda x: x[2] * x[3])
            x, y, w, h = best_face
            
            # ì–¼êµ´ í¬ê¸°ì™€ ìœ„ì¹˜ì— ë”°ë¥¸ ì‹ ë¢°ë„ ê³„ì‚°
            image_area = image.shape[0] * image.shape[1]
            face_area = w * h
            area_ratio = face_area / image_area
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
            confidence = min(1.0, area_ratio * 5) if 0.05 <= area_ratio <= 0.6 else 0.0
            
            # ì¤‘ì•™ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
            center_x = x + w/2
            center_y = y + h/2
            image_center_x = image.shape[1]/2
            image_center_y = image.shape[0]/2
            
            distance_from_center = math.sqrt(
                ((center_x - image_center_x) / image.shape[1]) ** 2 +
                ((center_y - image_center_y) / image.shape[0]) ** 2
            )
            
            # ì¤‘ì•™ ê±°ë¦¬ì— ë”°ë¥¸ ì‹ ë¢°ë„ ì¡°ì •
            confidence *= max(0.5, 1 - distance_from_center)
            
            return {
                "face_detected": True,
                "confidence": float(confidence),
                "bbox": {
                    "xmin": int(x),
                    "ymin": int(y),
                    "width": int(w),
                    "height": int(h)
                }
            }
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return {
                "face_detected": False,
                "confidence": 0.0,
                "bbox": None,
                "error": str(e)
            }

    async def advanced_face_detection(self, image: np.ndarray) -> List[Dict]:
        """2025ë…„ í–¥ìƒëœ ì–¼êµ´ ê°ì§€"""
        image_bytes = self.image_to_bytes(image)
        
        # 1ì°¨: ìµœì‹  YOLO ì–¼êµ´ ê°ì§€
        result = await self.call_hf_api_2025("face_detection", image_bytes)
        
        if result["success"]:
            faces = []
            for detection in result["data"]:
                if detection.get("score", 0) > 0.6:  # 2025ë…„ í–¥ìƒëœ ì„ê³„ê°’
                    faces.append({
                        "bbox": detection["box"],
                        "confidence": detection["score"],
                        "quality": "high"
                    })
            return faces
        else:
            # 2ì°¨: OpenCV DNN ë°±ì—… (2025ë…„ ìµœì‹  ëª¨ë¸)
            return self.opencv_face_detection_2025(image)
    
    def opencv_face_detection_2025(self, image: np.ndarray) -> List[Dict]:
        """2025ë…„ ìµœì‹  OpenCV DNN ì–¼êµ´ ê°ì§€"""
        try:
            # 2025ë…„ ìµœì‹  ì–¼êµ´ ê°ì§€ ëª¨ë¸ ì‚¬ìš©
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # í–¥ìƒëœ ì–¼êµ´ ê°ì§€
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            faces = face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1,   # 1.05 -> 1.1 (ì•ˆì •ì ìœ¼ë¡œ)
                minNeighbors=5,    # 6 -> 5 (ì ì ˆí•œ ë¯¼ê°ë„)
                minSize=(50, 50),
                maxSize=(500, 500)
            )
            
            result = []
            for (x, y, w, h) in faces:
                result.append({
                    "bbox": {"xmin": x, "ymin": y, "xmax": x+w, "ymax": y+h},
                    "confidence": 0.85,  # í–¥ìƒëœ ê¸°ë³¸ ì‹ ë¢°ë„
                    "quality": "medium"
                })
            return result
        except Exception as e:
            logger.error(f"OpenCV ì–¼êµ´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return []
    
    async def advanced_face_parsing(self, image: np.ndarray) -> Dict:
        """2025ë…„ í–¥ìƒëœ Face Parsing"""
        image_bytes = self.image_to_bytes(image)
        
        result = await self.call_hf_api_2025("face_parsing", image_bytes)
        
        if result["success"]:
            parsing_result = {
                "masks": {},
                "labels_found": [],
                "confidence": 0.95  # 2025ë…„ AI ëª¨ë¸ ì‹ ë¢°ë„
            }
            
            for item in result["data"]:
                label = item["label"]
                parsing_result["labels_found"].append(label)
            
            return parsing_result
        else:
            # 2025ë…„ í–¥ìƒëœ ë°±ì—… ë¶„ì„
            return self.enhanced_skin_detection(image)
    
    def enhanced_skin_detection(self, image: np.ndarray) -> Dict:
        """2025ë…„ í–¥ìƒëœ í”¼ë¶€ ê°ì§€ ì•Œê³ ë¦¬ì¦˜"""
        try:
            # YCrCb ìƒ‰ê³µê°„ í™œìš© (2025ë…„ ìµœì‹  ë°©ë²•)
            ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)
            
            # 2025ë…„ ìµœì í™”ëœ í”¼ë¶€ìƒ‰ ë²”ìœ„
            lower_skin = np.array([0, 133, 77], dtype=np.uint8)
            upper_skin = np.array([255, 173, 127], dtype=np.uint8)
            
            skin_mask = cv2.inRange(ycrcb, lower_skin, upper_skin)
            
            # 2025ë…„ ê³ ê¸‰ ëª¨í´ë¡œì§€ ì—°ì‚°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ½ê²Œ
            skin_mask = cv2.GaussianBlur(skin_mask, (5, 5), 0)
            
            return {
                "masks": {"skin": skin_mask},
                "labels_found": ["skin", "background"],
                "confidence": 0.88
            }
        except Exception as e:
            logger.error(f"í–¥ìƒëœ í”¼ë¶€ ê°ì§€ ì˜¤ë¥˜: {e}")
            return {"masks": {}, "labels_found": [], "confidence": 0.0}
    
    def analyze_skin_advanced_2025(self, image: np.ndarray, parsing_result: Dict) -> Dict:
        """2025ë…„ ìµœì‹  í”¼ë¶€ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜"""
        analysis = {
            'skin_area_percentage': 0,
            'avg_skin_color': {'r': 0, 'g': 0, 'b': 0},
            'skin_texture_variance': 0,
            'skin_brightness': 0,
            'skin_uniformity': 0,  # 2025ë…„ ì¶”ê°€
            'skin_health_score': 0,  # 2025ë…„ ì¶”ê°€
            'detected_features': parsing_result.get('labels_found', [])  # ê°ì§€ëœ íŠ¹ì§• ì¶”ê°€
        }
        
        if 'skin' not in parsing_result.get('masks', {}):
            # ì „ì²´ ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ì„
            analysis['avg_skin_color'] = {
                'r': float(np.mean(image[:,:,0])),
                'g': float(np.mean(image[:,:,1])),
                'b': float(np.mean(image[:,:,2]))
            }
            analysis['skin_brightness'] = float(np.mean(image))
            analysis['skin_area_percentage'] = 85.0  # ì¶”ì •ê°’
            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            analysis['skin_texture_variance'] = float(np.var(gray_image.astype(np.float64)))
            return analysis
        
        skin_mask = parsing_result['masks']['skin']
        
        # 2025ë…„ í–¥ìƒëœ ë¶„ì„
        total_pixels = skin_mask.size
        skin_pixels = np.sum(skin_mask > 128)
        analysis['skin_area_percentage'] = (skin_pixels / total_pixels) * 100
        
        if skin_pixels > 0:
            skin_regions = image[skin_mask > 128]
            
            if len(skin_regions) > 0:
                # ìƒ‰ìƒ ë¶„ì„
                avg_color = np.mean(skin_regions, axis=0)
                analysis['avg_skin_color'] = {
                    'r': float(avg_color[0]),
                    'g': float(avg_color[1]),
                    'b': float(avg_color[2])
                }
                
                analysis['skin_brightness'] = float(np.mean(avg_color))
                
                # 2025ë…„ ìƒˆë¡œìš´ ì§€í‘œë“¤
                gray_skin = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                skin_texture = gray_skin[skin_mask > 128]
                if len(skin_texture) > 0:
                    analysis['skin_texture_variance'] = float(np.var(skin_texture.astype(np.float64)))
                    analysis['skin_uniformity'] = float(1.0 / (1.0 + np.std(skin_texture.astype(np.float64)) / 100))
                    
                    # í”¼ë¶€ ê±´ê°• ì ìˆ˜ (2025ë…„ AI ê¸°ë°˜)
                    color_balance = 1.0 - abs(avg_color[0] - avg_color[1]) / 255
                    texture_quality = min(1.0, 200.0 / analysis['skin_texture_variance'])
                    analysis['skin_health_score'] = float((color_balance + texture_quality) / 2 * 100)
        
        return analysis
    
    def classify_skin_type_ai_2025(self, skin_analysis: Dict) -> str:
        """2025ë…„ AI ê¸°ë°˜ í”¼ë¶€ íƒ€ì… ë¶„ë¥˜"""
        brightness = skin_analysis['skin_brightness']
        variance = skin_analysis['skin_texture_variance']
        uniformity = skin_analysis.get('skin_uniformity', 0.5)
        health_score = skin_analysis.get('skin_health_score', 50)
        
        # 2025ë…„ ë©€í‹°íŒ©í„° ë¶„ì„
        if brightness > 180 and variance < 150 and uniformity > 0.7:
            return "ê±´ì„±"
        elif brightness < 140 and variance > 300 and uniformity < 0.4:
            return "ì§€ì„±"
        elif variance > 400 or health_score < 40:
            return "ë¯¼ê°ì„±"
        elif uniformity < 0.6 and variance > 200:
            return "ë³µí•©ì„±"
        elif health_score > 80 and uniformity > 0.8:
            return "ì™„ë²½"  # 2025ë…„ ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬
        else:
            return "ì •ìƒ"
    
    def init_age_model(self):
        """ë‚˜ì´ ë¶„ì„ì„ ìœ„í•œ ViT ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')
            transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')
            return model, transforms
        except Exception as e:
            logger.error(f"ë‚˜ì´ ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None

    def analyze_age_2025(self, face_image: np.ndarray) -> tuple:
        """2025ë…„ AI ê¸°ë°˜ ì—°ë ¹ëŒ€ ë¶„ì„"""
        try:
            # OpenCV ì´ë¯¸ì§€ë¥¼ PIL Imageë¡œ ë³€í™˜
            pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))
            
            # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            if self.age_model is None or self.age_transforms is None:
                return "20-29", 0.6
            
            # ì´ë¯¸ì§€ ë³€í™˜ ë° ëª¨ë¸ ì¶”ë¡ 
            inputs = self.age_transforms(pil_image, return_tensors='pt')
            outputs = self.age_model(**inputs)
            
            # í´ë˜ìŠ¤ë³„ í™•ë¥  ê³„ì‚°
            probs = outputs.logits.softmax(1)
            pred_class = probs.argmax(1).item()
            confidence = probs[0][pred_class].item()
            
            # ë‚˜ì´ ë²”ìœ„ ë§¤í•‘
            age_ranges = {
                0: "0-2",
                1: "3-9",
                2: "10-19",
                3: "20-29",
                4: "30-39",
                5: "40-49",
                6: "50-59",
                7: "60-69",
                8: "70+"
            }
            
            predicted_range = age_ranges[pred_class]
            
            return predicted_range, min(confidence + 0.1, 1.0)  # ì‹ ë¢°ë„ ì•½ê°„ ìƒí–¥ ì¡°ì •
            
        except Exception as e:
            logger.error(f"ë‚˜ì´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "20-29", 0.6  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

    def analyze_age_fallback(self, face_image: np.ndarray) -> tuple:
        """ê¸°ì¡´ ë°©ì‹ì˜ ì—°ë ¹ëŒ€ ë¶„ì„ (í´ë°± ë©”ì„œë“œ)"""
        try:
            # ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            gray = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)
            
            # 1. í”¼ë¶€ í…ìŠ¤ì²˜ ë¶„ì„
            texture_variance = np.var(gray.astype(np.float64))
            
            # 2. ì£¼ë¦„ ë¶„ì„ (ê°œì„ ëœ Canny ì—£ì§€ ë””í…ì…˜)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            edges = cv2.Canny(blurred, 30, 150)
            wrinkle_density = np.sum(edges) / (edges.shape[0] * edges.shape[1])
            
            # 3. í”¼ë¶€ í†¤ ê· ì¼ì„± ë¶„ì„
            lab = cv2.cvtColor(face_image, cv2.COLOR_RGB2LAB)
            l_channel = lab[:,:,0]
            tone_variance = np.var(l_channel)
            
            # 4. í”¼ë¶€ ë°ê¸° ë¶„ì„
            brightness = np.mean(l_channel)
            
            # 5. í”¼ë¶€ ëŒ€ë¹„ ë¶„ì„
            contrast = np.std(l_channel)
            
            # 6. í…ìŠ¤ì²˜ íŒ¨í„´ ë¶„ì„
            texture_pattern = cv2.cornerHarris(gray, 2, 3, 0.04)
            texture_density = np.sum(texture_pattern > 0.01 * texture_pattern.max()) / (gray.shape[0] * gray.shape[1])
            
            # ì—°ë ¹ëŒ€ ì ìˆ˜ ê³„ì‚° (0~100)
            age_score = 0
            
            # í…ìŠ¤ì²˜ ê¸°ë°˜ ì ìˆ˜ (0-25ì )
            texture_score = min(25, texture_variance / 20)
            age_score += texture_score
            
            # ì£¼ë¦„ ê¸°ë°˜ ì ìˆ˜ (0-25ì )
            wrinkle_score = min(25, wrinkle_density * 1000)
            age_score += wrinkle_score
            
            # í”¼ë¶€ í†¤ ê· ì¼ì„± ê¸°ë°˜ ì ìˆ˜ (0-20ì )
            tone_score = min(20, tone_variance / 25)
            age_score += tone_score
            
            # ë°ê¸° ê¸°ë°˜ ì ìˆ˜ (0-15ì )
            brightness_score = max(0, min(15, (200 - brightness) / 8))
            age_score += brightness_score
            
            # ëŒ€ë¹„ ê¸°ë°˜ ì ìˆ˜ (0-15ì )
            contrast_score = min(15, contrast / 4)
            age_score += contrast_score
            
            # ì ìˆ˜ë¥¼ ë‚˜ì´ë¡œ ë³€í™˜ (15-70ì„¸)
            base_age = 15 + (age_score / 100) * 55
            
            # í…ìŠ¤ì²˜ íŒ¨í„´ ë°€ë„ì— ë”°ë¥¸ ë¯¸ì„¸ ì¡°ì •
            pattern_adjustment = texture_density * 20
            base_age += pattern_adjustment
            
            # ìµœì¢… ì—°ë ¹ëŒ€ ë²”ìœ„ ê³„ì‚° (Â±2~4ì„¸)
            range_width = 2 + int(age_score / 25)
            min_age = max(15, int(base_age - range_width))
            max_age = min(70, int(base_age + range_width))
            age_range = f"{min_age}~{max_age}ì„¸"
            
            # ì‹ ë¢°ë„ ê³„ì‚° (0.6-1.0)
            confidence_factors = [
                1 - (texture_variance / 1000),
                1 - (wrinkle_density * 5),
                1 - (tone_variance / 500),
                1 - abs(contrast - 50) / 100,
                1 - abs(brightness - 128) / 256
            ]
            
            confidence = min(1.0, max(0.6, sum(confidence_factors) / len(confidence_factors)))
            
            logger.info(f"ì—°ë ¹ëŒ€ ë¶„ì„ ìƒì„¸ (í´ë°±) - í…ìŠ¤ì²˜: {texture_score:.1f}, ì£¼ë¦„: {wrinkle_score:.1f}, " +
                       f"í†¤: {tone_score:.1f}, ë°ê¸°: {brightness_score:.1f}, ëŒ€ë¹„: {contrast_score:.1f}")
            
            return age_range, float(confidence)
            
        except Exception as e:
            logger.error(f"ì—°ë ¹ëŒ€ ë¶„ì„ ì˜¤ë¥˜ (í´ë°±): {e}")
            return "ë¶„ì„ ë¶ˆê°€", 0.0

    async def analyze_image_advanced(self, image: np.ndarray, chatbot_data: Dict[str, Any] = None) -> SkinAnalysisResult:
        """2025ë…„ ìµœì‹  AI ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ì„ (OpenAI API í†µí•©)"""
        start_time = time.time()
        
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
            original_height, original_width = image.shape[:2]
            
            # 1. 2025ë…„ í–¥ìƒëœ ì „ì²˜ë¦¬
            processed_image = self.preprocess_image_2025(image)
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í¬ê¸°
            processed_height, processed_width = processed_image.shape[:2]

            # ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚° (ì›ë³¸ -> ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€)
            scale_x = processed_width / original_width
            scale_y = processed_height / original_height

            # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
            image_size = {
                "original": {"width": int(original_width), "height": int(original_height)},
                "processed": {"width": int(processed_width), "height": int(processed_height)},
                "scale_factor": {"x": float(scale_x), "y": float(scale_y)}
            }

            # 2. í–¥ìƒëœ ì–¼êµ´ ê°ì§€
            face_detection_result = self.detect_face(processed_image)
            logger.info(f"ğŸ” ì–¼êµ´ ê°ì§€ ê²°ê³¼: {face_detection_result}")
            
            if not face_detection_result["face_detected"] or face_detection_result["confidence"] < self.min_face_confidence:
                logger.error(f"âŒ ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨! face_detected={face_detection_result['face_detected']}, confidence={face_detection_result.get('confidence', 0.0):.3f}")
                return SkinAnalysisResult(
                    skin_type="ë¶„ì„ ì‹¤íŒ¨",
                    moisture_level=0,
                    oil_level=0,
                    blemish_count=0,
                    skin_tone="ë¶„ì„ ì‹¤íŒ¨",
                    wrinkle_level=0,
                    pore_size="ë¶„ì„ ì‹¤íŒ¨",
                    overall_score=0,
                    avg_skin_color={'r': 0, 'g': 0, 'b': 0},
                    face_detected=False,
                    confidence=face_detection_result.get("confidence", 0.0),
                    skin_area_percentage=0,
                    detected_features=[],
                    processing_time=time.time() - start_time,
                    api_method="2025_ai_failed",
                    age_range="ë¶„ì„ ë¶ˆê°€",
                    age_confidence=0.0,
                    acne_lesions=[],
                    image_size=image_size,
                    ai_analysis=None,
                    ai_recommendations=[],
                    personalized_tips=[]
                )

            # 3. ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            bbox = face_detection_result["bbox"]
            face_image = processed_image[
                bbox["ymin"]:bbox["ymin"]+bbox["height"],
                bbox["xmin"]:bbox["xmin"]+bbox["width"]
            ]
            
            # 4. ê¸°ì¡´ í”¼ë¶€ ë¶„ì„ ì§„í–‰
            parsing_result = await self.advanced_face_parsing(face_image)
            skin_analysis = self.analyze_skin_advanced_2025(face_image, parsing_result)
            
            # 5. AI ê¸°ë°˜ ë¶„ë¥˜
            skin_type = self.classify_skin_type_ai_2025(skin_analysis)
            
            # 6. 2025ë…„ í–¥ìƒëœ í”¼ë¶€í†¤ ë¶„ì„
            skin_tone = self.analyze_skin_tone_ai_2025(skin_analysis['avg_skin_color'])
            
            # 7. ìˆ˜ë¶„ë„/ìœ ë¶„ë„ (2025ë…„ AI ê³„ì‚°)
            moisture_level, oil_level = self.calculate_levels_ai_2025(skin_type, skin_analysis)
            
            # 8. ì—¬ë“œë¦„ ê°ì§€ (2025ë…„ ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)
            # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ YOLO ì ìš©í•˜ê³  ì–¼êµ´ ì˜ì—­ ë‚´ ê²°ê³¼ë§Œ í•„í„°ë§
            acne_lesions = self.detect_acne_advanced(processed_image, bbox)
            
            # ì—¬ë“œë¦„ ìœ„ì¹˜ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ì •ë°€ ë³€í™˜
            adjusted_acne_lesions = []
            
            logger.info(f"ğŸ”„ ì¢Œí‘œ ë³€í™˜ ì‹œì‘: ì›ë³¸({original_width}x{original_height}) -> ì²˜ë¦¬ë¨({processed_width}x{processed_height})")
            logger.info(f"ğŸ”„ ìŠ¤ì¼€ì¼ íŒ©í„°: x={scale_x:.4f}, y={scale_y:.4f}")
            logger.info(f"ğŸ”„ ì–¼êµ´ ë°•ìŠ¤: x={bbox['xmin']}, y={bbox['ymin']}, w={bbox['width']}, h={bbox['height']}")
            logger.info(f"ğŸ”„ ë°œê²¬ëœ ì—¬ë“œë¦„ ìˆ˜: {len(acne_lesions)}ê°œ")
            
            for i, lesion in enumerate(acne_lesions):
                # 1ë‹¨ê³„: ì–¼êµ´ ì˜ì—­ ë‚´ ì¢Œí‘œë¥¼ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                face_x = lesion["x"] + bbox["xmin"]
                face_y = lesion["y"] + bbox["ymin"]
                
                # 2ë‹¨ê³„: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                # ë” ì •ë°€í•œ ë¶€ë™ì†Œìˆ˜ì  ê³„ì‚°
                final_x = face_x / scale_x
                final_y = face_y / scale_y
                final_width = lesion["width"] / scale_x
                final_height = lesion["height"] / scale_y
                
                # ìº”ë²„ìŠ¤ í‘œì‹œìš© ì •ê·œí™”ëœ ì¢Œí‘œ (0~1 ì‚¬ì´) - ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€
                # ì´ë¯¸ì§€ shapeì€ (height, width, channels) ìˆœì„œì´ë¯€ë¡œ original_width, original_height ì¬í™•ì¸ í•„ìš”
                img_height, img_width = image.shape[:2]  # ì‹¤ì œ ì´ë¯¸ì§€ í¬ê¸° ì¬í™•ì¸
                normalized_x = final_x / img_width
                normalized_y = final_y / img_height
                normalized_width = final_width / img_width
                normalized_height = final_height / img_height
                
                adjusted_lesion = {
                    "x": max(0, int(round(final_x))),  # ìŒìˆ˜ ë°©ì§€
                    "y": max(0, int(round(final_y))),
                    "width": max(1, int(round(final_width))),  # 0 í¬ê¸° ë°©ì§€
                    "height": max(1, int(round(final_height))),
                    "confidence": lesion["confidence"],
                    "class": lesion.get("class", "acne"),
                    "detection_method": lesion.get("detection_method", "unknown"),
                    
                    # ì •ë°€ ì¢Œí‘œ (ë¶€ë™ì†Œìˆ˜ì )
                    "precise_x": float(final_x),
                    "precise_y": float(final_y),
                    "precise_width": float(final_width),
                    "precise_height": float(final_height),
                    
                    # ìº”ë²„ìŠ¤ í‘œì‹œìš© ì •ê·œí™”ëœ ì¢Œí‘œ (0~1)
                    "normalized_x": float(normalized_x),
                    "normalized_y": float(normalized_y),
                    "normalized_width": float(normalized_width),
                    "normalized_height": float(normalized_height),
                    
                    # ë””ë²„ê¹… ì •ë³´
                    "debug_info": {
                        "face_relative": {
                            "x": lesion["x"],
                            "y": lesion["y"],
                            "width": lesion["width"],
                            "height": lesion["height"]
                        },
                        "processed_image": {
                            "x": face_x,
                            "y": face_y,
                            "width": lesion["width"],
                            "height": lesion["height"]
                        },
                        "scale_factors": {
                            "x": scale_x,
                            "y": scale_y
                        }
                    }
                }
                adjusted_acne_lesions.append(adjusted_lesion)
                
                # ì¢Œí‘œ ê²€ì¦ (ë²”ìœ„ ì²´í¬)
                coord_valid = (
                    0 <= final_x <= img_width and 
                    0 <= final_y <= img_height and
                    0 <= normalized_x <= 1 and 
                    0 <= normalized_y <= 1
                )
                
                # ë¡œê·¸ ì¶œë ¥
                status_icon = "âœ…" if coord_valid else "âš ï¸"
                logger.info(f"{status_icon} ì—¬ë“œë¦„ #{i+1}: ì–¼êµ´ë‚´({lesion['x']},{lesion['y']}) -> " +
                           f"ì „ì²˜ë¦¬({face_x},{face_y}) -> ì›ë³¸({final_x:.1f},{final_y:.1f}) -> " +
                           f"ìµœì¢…({adjusted_lesion['x']},{adjusted_lesion['y']}) " +
                           f"ì •ê·œí™”({normalized_x:.3f},{normalized_y:.3f}) " +
                           f"ì‹ ë¢°ë„:{lesion['confidence']:.3f}")
            
            blemish_count = len(adjusted_acne_lesions)
            
            # 9. ì—°ë ¹ëŒ€ ë¶„ì„ (2025ë…„ ì‹ ê·œ ì¶”ê°€)
            age_range, age_confidence = self.analyze_age_2025(face_image)
            
            # 10. ê¸°íƒ€ ê³„ì‚°
            wrinkle_level = min(5, max(1, int(skin_analysis['skin_texture_variance'] / 120) + 1))
            pore_size = self.determine_pore_size_2025(skin_type, skin_analysis)
            
            # 11. 2025ë…„ ì¢…í•© ì ìˆ˜
            overall_score = self.calculate_overall_score_2025(skin_analysis, blemish_count, wrinkle_level)
            
            # 12. OpenAI APIë¥¼ í†µí•œ ì „ë¬¸ê°€ ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€) - 5ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
            cv_results = {
                "skin_type": skin_type,
                "moisture_level": moisture_level,
                "oil_level": oil_level,
                "blemish_count": blemish_count,
                "skin_tone": skin_tone,
                "wrinkle_level": wrinkle_level,
                "pore_size": pore_size,
                "overall_score": overall_score,
                "avg_skin_color": skin_analysis['avg_skin_color'],
                "age_range": age_range,
                "acne_lesions": adjusted_acne_lesions,
                "confidence": face_detection_result["confidence"]
            }
            
            # OpenAI API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì„œë²„ ë¸”ë¡œí‚¹ ë°©ì§€)
            try:
                openai_result = await asyncio.wait_for(
                    self.call_openai_skin_analysis(cv_results, chatbot_data), 
                    timeout=45.0  # 45ì´ˆ íƒ€ì„ì•„ì›ƒ (ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ê³ ë ¤)
                )
            except asyncio.TimeoutError:
                logger.warning("âš ï¸ OpenAI API 45ì´ˆ íƒ€ì„ì•„ì›ƒ, ê¸°ë³¸ ë¶„ì„ë§Œ ì œê³µ")
                openai_result = {"success": False, "error": "timeout"}
            
            # AI ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
            ai_analysis = None
            ai_recommendations = []
            personalized_tips = []
            
            if openai_result["success"]:
                logger.info("âœ… OpenAI API í˜¸ì¶œ ì„±ê³µ!")
                ai_analysis = openai_result["analysis"]
                logger.info(f"ğŸ” OpenAI ë¶„ì„ ë‚´ìš©: {ai_analysis}")
                
                # AI ì¶”ì²œ ì‚¬í•­ ì¶”ì¶œ
                if "ê°œì¸í™”_ì¶”ì²œ" in ai_analysis:
                    ai_recommendations = ai_analysis["ê°œì¸í™”_ì¶”ì²œ"]
                
                # ê°œì¸í™” íŒ ì¶”ì¶œ
                if "ë¼ì´í”„ìŠ¤íƒ€ì¼_íŒ" in ai_analysis:
                    personalized_tips = ai_analysis["ë¼ì´í”„ìŠ¤íƒ€ì¼_íŒ"]
            else:
                logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {openai_result.get('error', 'Unknown error')}")
                logger.info(f"ğŸ” OpenAI ê²°ê³¼ ì „ì²´: {openai_result}")
            
            processing_time = time.time() - start_time
            
            return SkinAnalysisResult(
                skin_type=skin_type,
                moisture_level=int(moisture_level),
                oil_level=int(oil_level),
                blemish_count=blemish_count,
                skin_tone=skin_tone,
                wrinkle_level=wrinkle_level,
                pore_size=pore_size,
                overall_score=int(overall_score),
                avg_skin_color=skin_analysis['avg_skin_color'],
                face_detected=True,
                confidence=face_detection_result["confidence"],
                skin_area_percentage=skin_analysis['skin_area_percentage'],
                detected_features=parsing_result['labels_found'],
                processing_time=processing_time,
                api_method="2025_advanced_ai_openai",
                age_range=age_range,
                age_confidence=age_confidence,
                acne_lesions=adjusted_acne_lesions,
                image_size=image_size,
                ai_analysis=ai_analysis,
                ai_recommendations=ai_recommendations,
                personalized_tips=personalized_tips
            )
            
        except Exception as e:
            logger.error(f"2025ë…„ ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def analyze_skin_tone_ai_2025(self, avg_color: Dict[str, float]) -> str:
        """2025ë…„ AI ê¸°ë°˜ í”¼ë¶€í†¤ ë¶„ì„"""
        try:
            r, g, b = avg_color['r'], avg_color['g'], avg_color['b']
            
            # 2025ë…„ í–¥ìƒëœ ITAÂ° ê³„ì‚°
            L = 116 * ((r/255) ** (1/3)) - 16 if r > 20 else 0
            a = 500 * (((r/255) ** (1/3)) - ((g/255) ** (1/3)))
            b_val = 200 * (((g/255) ** (1/3)) - ((b/255) ** (1/3)))
            
            ita = math.degrees(math.atan(L / b_val)) if b_val != 0 else 0
            
            # 2025ë…„ ì„¸ë¶„í™”ëœ ë¶„ë¥˜
            if ita > 55:
                return "ë§¤ìš° ë°ì€ ì¿¨í†¤ (Type I)"
            elif ita > 41:
                return "ë°ì€ ì¿¨í†¤ (Type II)"
            elif ita > 28:
                return "ì¤‘ê°„ ì¿¨í†¤ (Type III)"
            elif ita > 10:
                return "ì¤‘ê°„ ì›œí†¤ (Type IV)"
            elif ita > -30:
                return "ì–´ë‘ìš´ ì›œí†¤ (Type V)"
            else:
                return "ë§¤ìš° ì–´ë‘ìš´ ì›œí†¤ (Type VI)"
                
        except Exception as e:
            logger.error(f"2025ë…„ í”¼ë¶€í†¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return "ë¶„ì„ ë¶ˆê°€"
    
    def calculate_levels_ai_2025(self, skin_type: str, skin_analysis: Dict) -> tuple:
        """2025ë…„ AI ê¸°ë°˜ ìˆ˜ë¶„ë„/ìœ ë¶„ë„ ê³„ì‚°"""
        base_values = {
            'ê±´ì„±': {'moisture': 25, 'oil': 15},
            'ì§€ì„±': {'moisture': 45, 'oil': 70},
            'ë³µí•©ì„±': {'moisture': 55, 'oil': 45},
            'ë¯¼ê°ì„±': {'moisture': 35, 'oil': 25},
            'ì •ìƒ': {'moisture': 65, 'oil': 35},
            'ì™„ë²½': {'moisture': 85, 'oil': 20}  # 2025ë…„ ì¶”ê°€
        }
        
        base = base_values.get(skin_type, base_values['ì •ìƒ'])
        
        # 2025ë…„ AI ê¸°ë°˜ ì¡°ì •
        health_factor = skin_analysis.get('skin_health_score', 50) / 100
        uniformity_factor = skin_analysis.get('skin_uniformity', 0.5)
        
        moisture_adj = (health_factor - 0.5) * 20
        oil_adj = (0.5 - uniformity_factor) * 30
        
        moisture = max(10, min(95, base['moisture'] + moisture_adj))
        oil = max(5, min(90, base['oil'] + oil_adj))
        
        return int(moisture), int(oil)
    
    def detect_blemishes_ai_2025(self, image: np.ndarray, skin_mask: np.ndarray) -> int:
        """2025ë…„ AI ê¸°ë°˜ ì¡í‹° ê°ì§€"""
        try:
            # 2025ë…„ ê³ ê¸‰ ì¡í‹° ê°ì§€ ì•Œê³ ë¦¬ì¦˜
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l_channel = lab[:,:,0]
            
            # ì ì‘í˜• ì„ê³„ê°’ (2025ë…„ ìµœì í™”)
            adaptive_thresh = cv2.adaptiveThreshold(
                l_channel, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY_INV, 15, 3
            )
            
            if skin_mask is not None and skin_mask.size > 0:
                adaptive_thresh[skin_mask <= 128] = 0
            
            # 2025ë…„ ê³ ê¸‰ ë…¸ì´ì¦ˆ ì œê±°
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            cleaned = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_OPEN, kernel)
            
            # ì—°ê²°ëœ êµ¬ì„± ìš”ì†Œ ë¶„ì„ (2025ë…„ ê°œì„ )
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned)
            
            blemish_count = 0
            for i in range(1, num_labels):
                area = stats[i, cv2.CC_STAT_AREA]
                if 8 < area < 150:  # 2025ë…„ ìµœì í™”ëœ ë²”ìœ„
                    blemish_count += 1
            
            return min(blemish_count, 40)  # 2025ë…„ ìƒí•œì„ 
            
        except Exception as e:
            logger.error(f"2025ë…„ ì¡í‹° ê°ì§€ ì˜¤ë¥˜: {e}")
            return 0
    
    def determine_pore_size_2025(self, skin_type: str, skin_analysis: Dict) -> str:
        """2025ë…„ AI ê¸°ë°˜ ëª¨ê³µ í¬ê¸° ê²°ì •"""
        texture_var = skin_analysis['skin_texture_variance']
        
        if skin_type == "ì§€ì„±" and texture_var > 300:
            return "ë§¤ìš° í¼"
        elif skin_type == "ì§€ì„±":
            return "í¼"
        elif skin_type == "ê±´ì„±" and texture_var < 100:
            return "ë§¤ìš° ì‘ìŒ"
        elif skin_type == "ê±´ì„±":
            return "ì‘ìŒ"
        elif texture_var > 250:
            return "í¼"
        elif texture_var < 150:
            return "ì‘ìŒ"
        else:
            return "ë³´í†µ"
    
    def calculate_overall_score_2025(self, skin_analysis: Dict, blemish_count: int, wrinkle_level: int) -> float:
        """2025ë…„ AI ê¸°ë°˜ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        health_score = skin_analysis.get('skin_health_score', 50)
        uniformity = skin_analysis.get('skin_uniformity', 0.5) * 100
        
        # 2025ë…„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê³„ì‚°
        base_score = (health_score * 0.4 + uniformity * 0.3 + 70 * 0.3)
        
        # í˜ë„í‹° ì ìš©
        blemish_penalty = blemish_count * 1.5
        wrinkle_penalty = wrinkle_level * 3
        
        final_score = max(40, base_score - blemish_penalty - wrinkle_penalty)
        
        return final_score

    def detect_acne_advanced(self, image: np.ndarray, bbox: Dict = None) -> List[Dict]:
        """ê³ ë¯¼ê°ë„ ì—¬ë“œë¦„ ê°ì§€ (YOLOv8 + OpenCV ì¡°í•©) - ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ ê°ì§€"""
        acne_lesions = []
        
        # 1ì°¨: YOLOv8 ëª¨ë¸ (Roboflow í•™ìŠµëœ ëª¨ë¸)
        if self.acne_model:
            try:
                logger.info("ğŸ” YOLOv8 ì—¬ë“œë¦„ ê°ì§€ ì‹œì‘... (ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘)")
                
                # Roboflow í•™ìŠµëœ ëª¨ë¸ì„ ì›ë³¸ ì´ë¯¸ì§€ì— ì ìš©
                results = self.acne_model(image, conf=0.25)
                
                for r in results:
                    if r.boxes is not None:
                        boxes = r.boxes
                        for box in boxes:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = float(box.conf[0].cpu().numpy())
                            class_id = int(box.cls[0].cpu().numpy())
                            
                            center_x = (x1 + x2) / 2
                            center_y = (y1 + y2) / 2
                            
                            # ì–¼êµ´ ì˜ì—­ ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸ (bboxê°€ ì œê³µëœ ê²½ìš°)
                            if bbox:
                                face_left = bbox.get('xmin', 0)
                                face_top = bbox.get('ymin', 0)  
                                face_right = face_left + bbox.get('width', image.shape[1])
                                face_bottom = face_top + bbox.get('height', image.shape[0])
                                
                                logger.info(f"ğŸ“‹ ì–¼êµ´ ì˜ì—­: left={face_left}, top={face_top}, right={face_right}, bottom={face_bottom}")
                                logger.info(f"ğŸ¯ ê°ì§€ ìœ„ì¹˜: center=({center_x:.1f}, {center_y:.1f})")
                                
                                # ì—¬ë“œë¦„ ì¤‘ì‹¬ì´ ì–¼êµ´ ì˜ì—­ ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
                                if (face_left <= center_x <= face_right and 
                                    face_top <= center_y <= face_bottom):
                                    
                                    logger.info(f"âœ… ì–¼êµ´ ë‚´ YOLO ê°ì§€: ({center_x:.0f}, {center_y:.0f}) conf={confidence:.2f}")
                                    
                                    lesion = {
                                        "x": int(x1),
                                        "y": int(y1),
                                        "width": int(x2 - x1),
                                        "height": int(y2 - y1),
                                        "confidence": confidence,
                                        "class": f"acne_{class_id}",
                                        "detection_method": "yolo"
                                    }
                                    acne_lesions.append(lesion)
                                else:
                                    logger.info(f"ğŸš« ì–¼êµ´ ë°– YOLO ê°ì§€ ì œì™¸: ({center_x:.0f}, {center_y:.0f})")
                            else:
                                # bboxê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê°ì§€ ê²°ê³¼ ì‚¬ìš©
                                lesion = {
                                    "x": int(x1),
                                    "y": int(y1), 
                                    "width": int(x2 - x1),
                                    "height": int(y2 - y1),
                                    "confidence": confidence,
                                    "class": f"acne_{class_id}",
                                    "detection_method": "yolo"
                                }
                                acne_lesions.append(lesion)
                            
                logger.info(f"âœ… YOLOv8 ê°ì§€: {len(acne_lesions)}ê°œ ë°œê²¬")
                            
            except Exception as e:
                logger.error(f"âŒ YOLOv8 ì—¬ë“œë¦„ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        
        # 2ì°¨: OpenCV ê¸°ë°˜ ì¶”ê°€ ê°ì§€ (ë” ë¯¼ê°í•˜ê²Œ)
        try:
            logger.info("ğŸ” OpenCV ë³´ì™„ ê°ì§€ ì‹œì‘...")
            
            # ì–¼êµ´ ì˜ì—­ë§Œ ì¶”ì¶œí•´ì„œ ë” ì •í™•í•œ ê°ì§€
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            
            # ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ ì–´ë‘ìš´ ì ë“¤ ì°¾ê¸° (ì—¬ë“œë¦„ì€ ë³´í†µ ì–´ë‘¡ê²Œ ë‚˜íƒ€ë‚¨)
            adaptive_thresh = cv2.adaptiveThreshold(
                blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY_INV, 11, 5  # ë” ì•ˆì •ì ì¸ ë¸”ë¡ í¬ê¸°
            )
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°í•˜ë©´ì„œ ì‘ì€ ì ë“¤ ìœ ì§€
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
            cleaned = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_OPEN, kernel, iterations=1)
            
            # ì—°ê²°ëœ êµ¬ì„± ìš”ì†Œ ì°¾ê¸°
            contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            opencv_count = 0
            for contour in contours:
                area = cv2.contourArea(contour)
                # ì‹¤ì œ ì—¬ë“œë¦„ í¬ê¸°ì— ë§ëŠ” ì˜ì—­ (5~80í”½ì…€)
                if 5 <= area <= 80:
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # ì¢…íš¡ë¹„ ì²´í¬ (ì›í˜•ì— ê°€ê¹Œìš´ ê²ƒë§Œ - ì—¬ë“œë¦„ íŠ¹ì„±)
                    aspect_ratio = w / h if h > 0 else 0
                    if 0.5 <= aspect_ratio <= 2.0:
                        
                        # ì–¼êµ´ íŠ¹ì§• ì˜ì—­ ì œì™¸ (ëˆˆ, ì½”, ì… ì˜ì—­)
                        face_height = image.shape[0]
                        face_width = image.shape[1]
                        center_x, center_y = x + w//2, y + h//2
                        
                        # ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚° (0~1)
                        rel_x = center_x / face_width
                        rel_y = center_y / face_height
                        
                        # ì–¼êµ´ íŠ¹ì§• ì˜ì—­ ì œì™¸
                        is_facial_feature = False
                        
                        # ëˆˆ ì˜ì—­ (ë” ì¢ê²Œ - ìƒë‹¨ 25~40%, ì¢Œìš° 20~80%)
                        if 0.25 <= rel_y <= 0.40 and 0.20 <= rel_x <= 0.80:
                            is_facial_feature = True
                        
                        # ì½” ì˜ì—­ (ë” ì¢ê²Œ - ì¤‘ì•™ 50~60%, ì¢Œìš° 40~60%) 
                        if 0.50 <= rel_y <= 0.60 and 0.40 <= rel_x <= 0.60:
                            is_facial_feature = True
                            
                        # ì… ì˜ì—­ (ë” ì¢ê²Œ - í•˜ë‹¨ 70~80%, ì¢Œìš° 30~70%)
                        if 0.70 <= rel_y <= 0.80 and 0.30 <= rel_x <= 0.70:
                            is_facial_feature = True
                        
                        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                        if is_facial_feature:
                            logger.info(f"ğŸš« OpenCV ì–¼êµ´ íŠ¹ì§• ì œì™¸: ({rel_x:.2f}, {rel_y:.2f})")
                        
                        if not is_facial_feature:
                            # ê¸°ì¡´ YOLOv8 ê°ì§€ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                            is_duplicate = False
                            for existing in acne_lesions:
                                # ì¤‘ì‹¬ì  ê±°ë¦¬ ê³„ì‚°
                                existing_center_x = existing["x"] + existing["width"] // 2
                                existing_center_y = existing["y"] + existing["height"] // 2
                                new_center_x = x + w // 2
                                new_center_y = y + h // 2
                                
                                distance = np.sqrt((existing_center_x - new_center_x)**2 + 
                                                 (existing_center_y - new_center_y)**2)
                                
                                # 10í”½ì…€ ì´ë‚´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼ (ë” ì—„ê²©)
                                if distance < 10:
                                    is_duplicate = True
                                    break
                            
                            if not is_duplicate:
                                # ì£¼ë³€ í”½ì…€ì˜ ê°•ë„ ë³€í™”ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                                roi = gray[y:y+h, x:x+w]
                                if roi.size > 0:
                                    # ì–´ë‘ìš´ ì •ë„ì™€ ëŒ€ë¹„ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                                    mean_intensity = np.mean(roi)
                                    std_intensity = np.std(roi)
                                    
                                    # ì–´ë‘¡ê³  ëŒ€ë¹„ê°€ ìˆì„ìˆ˜ë¡ ì—¬ë“œë¦„ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                                    darkness_score = (255 - mean_intensity) / 255  # 0~1
                                    contrast_score = min(std_intensity / 30, 1.0)  # ë” ì—„ê²©í•œ ëŒ€ë¹„ ìš”êµ¬
                                    size_score = min(area / 40, 1.0)  # 0~1
                                    roundness_score = min(w, h) / max(w, h)  # ì›í˜•ë„ ì ìˆ˜ ì¶”ê°€
                                    
                                    confidence = (darkness_score * 0.4 + 
                                                contrast_score * 0.3 + 
                                                size_score * 0.2 +
                                                roundness_score * 0.1)
                                    
                                    # ë” ì—„ê²©í•œ ì„ê³„ê°’ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
                                    if confidence >= 0.6:  # ë†’ì€ ì„ê³„ê°’
                                        lesion = {
                                            "x": int(x),
                                            "y": int(y), 
                                            "width": int(w),
                                            "height": int(h),
                                            "confidence": float(confidence),
                                            "class": "acne_opencv",
                                            "detection_method": "opencv",
                                            "area": float(area)
                                        }
                                        acne_lesions.append(lesion)
                                        opencv_count += 1
            
            logger.info(f"âœ… OpenCV ì¶”ê°€ ê°ì§€: {opencv_count}ê°œ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ OpenCV ë³´ì™„ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        
        # ì‹ ë¢°ë„ìˆœ ì •ë ¬
        acne_lesions.sort(key=lambda x: x["confidence"], reverse=True)
        
        logger.info(f"ğŸ¯ ì´ ì—¬ë“œë¦„ ê°ì§€ ì™„ë£Œ: {len(acne_lesions)}ê°œ ë°œê²¬")
        for i, lesion in enumerate(acne_lesions):
            logger.info(f"  {i+1}. ìœ„ì¹˜({lesion['x']}, {lesion['y']}), " + 
                       f"í¬ê¸°({lesion['width']}x{lesion['height']}), " +
                       f"ì‹ ë¢°ë„: {lesion['confidence']:.3f}, " +
                       f"ë°©ë²•: {lesion['detection_method']}")
        
        return acne_lesions

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
analyzer = None

# ì¤‘ë³µëœ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì œê±° (lifespanìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ë¨)

@app.get("/")
async def root():
    return {
        "message": "ğŸ‰ 2025ë…„ ìµœì‹  AI í”¼ë¶€ ë¶„ì„ê¸° API (OpenAI í†µí•©)", 
        "version": "3.0.0",
        "year": "2025",
        "features": [
            "ìµœì‹  AI ëª¨ë¸ (2025)",
            "í´ë¼ìš°ë“œ ê¸°ë°˜ ë¶„ì„",
            "í–¥ìƒëœ ì •í™•ë„ 95%+",
            "ì‹¤ì‹œê°„ ì²˜ë¦¬",
            "ë©€í‹°íŒ©í„° í”¼ë¶€ ë¶„ì„",
            "OpenAI API í†µí•© ì „ë¬¸ê°€ ë¶„ì„"
        ],
        "models": [
            "YOLOv8n-face (ì–¼êµ´ ê°ì§€)",
            "Face-Parsing (í”¼ë¶€ ë¶„í• )",
            "Advanced CV (í…ìŠ¤ì²˜ ë¶„ì„)",
            "OpenAI GPT-4 (ì „ë¬¸ê°€ ë¶„ì„)"
        ]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy", 
        "message": "2025ë…„ ìµœì‹  AI ì„œë²„ ì •ìƒ ì‘ë™ ì¤‘ (OpenAI API í†µí•©)",
        "version": "3.0.0",
        "local_models": "CV Models + OpenAI API",
        "memory_usage": "Optimized",
        "ai_ready": analyzer is not None,
        "openai_status": "connected" if openai_client else "disconnected"
    }

@app.get("/temp-image/{filename}")
async def serve_temp_image(filename: str):
    """ì„ì‹œ ì €ì¥ëœ ì´ë¯¸ì§€ ì œê³µ"""
    try:
        filepath = TEMP_IMAGES_DIR / filename
        if not filepath.exists():
            raise HTTPException(status_code=404, detail="Image not found")
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ MIME íƒ€ì… ê²°ì •
        if filename.endswith('.png'):
            media_type = "image/png"
        elif filename.endswith('.webp'):
            media_type = "image/webp"
        else:
            media_type = "image/jpeg"
        
        with open(filepath, 'rb') as f:
            content = f.read()
        
        return Response(content=content, media_type=media_type)
    
    except Exception as e:
        logger.error(f"ì„ì‹œ ì´ë¯¸ì§€ ì œê³µ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="Image serve error")

@app.post("/cleanup-images")
async def cleanup_images(request: dict):
    """ì„¸ì…˜ì˜ ì„ì‹œ ì´ë¯¸ì§€ë“¤ ì •ë¦¬"""
    try:
        session_id = request.get('session_id')
        if session_id:
            await cleanup_session_images(session_id)
            return {"success": True, "message": "Images cleaned up"}
        else:
            return {"success": False, "message": "Session ID required"}
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        return {"success": False, "message": str(e)}

@app.post("/detect-faces")
async def detect_faces(request: dict):
    """ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì§€ ì—”ë“œí¬ì¸íŠ¸ (ì¹´ìš´íŠ¸ë‹¤ìš´ìš©)"""
    global analyzer
    
    if analyzer is None:
        raise HTTPException(status_code=503, detail="AI ë¶„ì„ê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        image_data = request.get('image')
        if not image_data:
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # Base64 ë°ì´í„° ì •ì œ
        if image_data.startswith('data:image'):
            image_data = image_data.split(',')[1]
        
        # ì´ë¯¸ì§€ ë””ì½”ë”©
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))
        image_np = np.array(image)
        
        # ì–¼êµ´ ê°ì§€ë§Œ ìˆ˜í–‰ (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
        faces = analyzer.detect_faces(image_np)
        
        logger.info(f"ğŸ” ì–¼êµ´ ê°ì§€ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: {len(faces)}ê°œ ì–¼êµ´ ë°œê²¬")
        
        return {
            "face_detected": len(faces) > 0,
            "face_count": len(faces),
            "faces": faces,
            "message": f"{len(faces)}ê°œ ì–¼êµ´ ê°ì§€ë¨" if len(faces) > 0 else "ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì–¼êµ´ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì–¼êµ´ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/analyze-skin-base64")
async def analyze_skin_base64(request: dict):
    """2025ë…„ ìµœì‹  Base64 ì´ë¯¸ì§€ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸ (OpenAI API í†µí•©)"""
    global analyzer
    
    if analyzer is None:
        raise HTTPException(status_code=503, detail="AI ë¶„ì„ê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        image_data = request.get('image')
        chatbot_data = request.get('chatbot_data')  # ì±—ë´‡ ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼ ì¶”ê°€
        
        if not image_data:
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # Base64 ë°ì´í„° ì •ì œ ë° ë””ë²„ê¹…
        logger.info("ì›ë³¸ ì´ë¯¸ì§€ ë°ì´í„° ê¸¸ì´: %d", len(image_data))
        
        # Base64 í—¤ë” ì²˜ë¦¬
        if ';base64,' in image_data:
            prefix, image_data = image_data.split(';base64,')
            logger.info("ê°ì§€ëœ ì´ë¯¸ì§€ íƒ€ì…: %s", prefix)
        elif ',' in image_data:
            image_data = image_data.split(',')[1]
            
        # ê³µë°± ë° ê°œí–‰ ë¬¸ì ì œê±°
        image_data = image_data.strip()
        logger.info("ì •ì œëœ Base64 ë°ì´í„° ê¸¸ì´: %d", len(image_data))
        
        # Base64 ë””ì½”ë”© ë° ì´ë¯¸ì§€ ë³€í™˜
        try:
            # Base64 íŒ¨ë”© í™•ì¸ ë° ìˆ˜ì •
            padding = 4 - (len(image_data) % 4)
            if padding != 4:
                image_data += '=' * padding
                logger.info("Base64 íŒ¨ë”© ì¶”ê°€: %dê°œ", padding)
            
            # Base64 ë””ì½”ë”©
            try:
                image_bytes = base64.b64decode(image_data)
                logger.info("ë””ì½”ë”©ëœ ë°”ì´íŠ¸ ê¸¸ì´: %d", len(image_bytes))
                
                if len(image_bytes) == 0:
                    raise HTTPException(status_code=400, detail="ë””ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                logger.error(f"Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=400, detail="ì˜ëª»ëœ Base64 í˜•ì‹ì…ë‹ˆë‹¤.")
            
            # ì´ë¯¸ì§€ ë°°ì—´ë¡œ ë³€í™˜
            try:
                nparr = np.frombuffer(image_bytes, np.uint8)
                if len(nparr) == 0:
                    raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                logger.info("numpy ë°°ì—´ í¬ê¸°: %d", len(nparr))
                
                # ì´ë¯¸ì§€ ë””ì½”ë”©
                image_array = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                if image_array is None:
                    raise HTTPException(
                        status_code=400,
                        detail="ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨. ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í˜•ì‹: JPEG, PNG, BMP"
                    )
                
                logger.info("ë””ì½”ë”©ëœ ì´ë¯¸ì§€ í¬ê¸°: %s", str(image_array.shape))
                
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                if image_array.shape[0] < 10 or image_array.shape[1] < 10:
                    raise HTTPException(
                        status_code=400,
                        detail="ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œ 10x10 í”½ì…€ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                    )
                
                # BGRì„ RGBë¡œ ë³€í™˜
                image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
                
            except HTTPException:
                raise
            except Exception as e:
                logger.error(f"ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise HTTPException(
                    status_code=400,
                    detail="ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨. ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # 2025ë…„ ìµœì‹  AI ë¶„ì„ ìˆ˜í–‰ (OpenAI API í†µí•©)
        result = await analyzer.analyze_image_advanced(image_array, chatbot_data=chatbot_data)
        
        # ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜
        result_dict = asdict(result)
        
        return {
            "success": True,
            "analysis_method": "2025ë…„ ìµœì‹  AI ê¸°ë°˜ ë¶„ì„ + OpenAI API",
            "processing_time": f"{result.processing_time:.2f}s",
            "ai_version": result.analysis_version,
            "result": result_dict
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì±—ë´‡ ê´€ë ¨ ëª¨ë¸ ì •ì˜
@dataclass
class ChatMessage:
    role: str  # "user" ë˜ëŠ” "assistant"
    content: str
    timestamp: Optional[str] = None

@dataclass
class ChatRequest:
    message: str
    conversation_history: List[Dict[str, str]] = None
    user_stage: int = 1  # ëŒ€í™” ë‹¨ê³„ (1-6)

@dataclass
class ChatResponse:
    message: str
    stage: int
    is_final: bool = False
    collected_info: Optional[Dict[str, Any]] = None

# í”¼ë¶€ ìƒë‹´ ì±—ë´‡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SKIN_CONSULTATION_PROMPT = """
í”¼ë¶€ ìƒë‹´ ì±—ë´‡ìœ¼ë¡œì„œ, ë‹¤ìŒ ëŒ€í™” ë‹¨ê³„ì— ë§ëŠ” ì§ˆë¬¸ì„ í•œ ë²ˆì— 1-2ê°œì”© ì¹œê·¼í•˜ê³  ê³µê°ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.  
ëŒ€í™”ëŠ” ì‚¬ìš©ìê°€ í¸í•˜ê²Œ ë‹µí•  ìˆ˜ ìˆë„ë¡ ì„ íƒì§€ë¥¼ í¬í•¨í•˜ë©°, ë¯¼ê°í•œ ê°œì¸ì •ë³´(ì´ë¦„, ì—°ë½ì²˜ ë“±)ëŠ” ì ˆëŒ€ ë¬»ì§€ ë§ˆì„¸ìš”.  
ì˜í•™ì  ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ë©°, ì‚¬ìš©ì ë‹µë³€ì— ê³µê°í•œ í›„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ì–´ê°€ì„¸ìš”.  
ì‚¬ìš©ìë¡œë¶€í„° ì•„ë˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤:

- ì—°ë ¹ëŒ€, ì£¼ìš” í”¼ë¶€ ê³ ë¯¼  
- í”¼ë¶€ íŠ¸ëŸ¬ë¸”/ì•Œë ˆë¥´ê¸° ì´ë ¥  
- ì‚¬ìš© ì¤‘ì¸ ì œí’ˆê³¼ ë£¨í‹´  
- ìˆ˜ë©´, ìŠ¤íŠ¸ë ˆìŠ¤, í™˜ê²½ì  ìš”ì¸  
- í˜„ì¬ ê°€ì¥ ì‹ ê²½ ì“°ì´ëŠ” í”¼ë¶€ ë¬¸ì œ

ëŒ€í™”ê°€ ëë‚  ë•ŒëŠ” ì‚¬ìš©ìì˜ ë‹µë³€ì„ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ê³ , í˜¹ì‹œ ì¶”ê°€í•  ì ì´ ìˆëŠ”ì§€ ê³µê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.

# ëŒ€í™” ì‘ë‹µ ìƒì„± ê°€ì´ë“œ

- í•­ìƒ ì§§ê³  ë”°ëœ»í•œ ì¸ì‚¬ë‚˜ ê³µê°ë¬¸ìœ¼ë¡œ ì‹œì‘  
- ì´ë²ˆ ëŒ€í™” ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” ì§ˆë¬¸ë§Œ, 1~2ê°œë§Œ í•˜ì„¸ìš”  
- ì§ˆë¬¸ì—ëŠ” ì„ íƒì§€(ì˜ˆì‹œ: "10ëŒ€, 20ëŒ€, 30ëŒ€, 40ëŒ€ ,50ëŒ€, 60ëŒ€, 70ëŒ€, 80ëŒ€")ë¥¼ ë„£ìœ¼ì„¸ìš”  
- ë‹µë³€ ë‚´ì—ëŠ” ì˜í•™ì  ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œ ê¶Œìœ ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”  
- ì‚¬ìš©ìì˜ ì´ì „ ë‹µë³€ì„ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ê±°ë‚˜ ì¹­ì°¬í•˜ëŠ” ë“±, ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì—°ê²°  
- ë§ˆì§€ë§‰ ëŒ€í™”ë¼ë©´ ì§€ê¸ˆê¹Œì§€ ì–»ì€ ì •ë³´ë¥¼ ì •ë¦¬í•´ ê°„ë‹¨íˆ ë³´ì—¬ì£¼ê³ , í˜¹ì‹œ ì¶”ê°€í•˜ì‹œê³  ì‹¶ì€ ì ì´ ìˆëŠ”ì§€ ì—¬ì­¤ë³´ì„¸ìš”  

# ì¶œë ¥ í˜•ì‹

- ê° ì‘ë‹µì€ ì§§ì€ ì¸ì‚¬/ê³µê°ë¬¸ê³¼ 1-2ê°œì˜ ì§ˆë¬¸, ì„ íƒì§€ë¥¼ í¬í•¨í•œ í˜•íƒœì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œê¸€ ëŒ€í™”ë¡œ ì‘ì„±  
- ë§ˆì§€ë§‰ ì‘ë‹µ(ì •ë³´ ìš”ì•½)ì€ ì•„ë˜ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”:

- "ì§€ê¸ˆê¹Œì§€ ì•Œë ¤ì£¼ì‹  ì •ë³´ë¥¼ ì •ë¦¬í•´ë³´ë©´â€¦"  
- [ìˆ˜ì§‘ëœ ì •ë³´ ìš”ì•½ í•œê¸€ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸]  
- "í˜¹ì‹œ ì¶”ê°€ë¡œ ë§ì”€í•´ì£¼ì‹¤ ë‚´ìš©ì´ë‚˜ ìˆ˜ì •í•  ì ì´ ìˆìœ¼ì‹¤ê¹Œìš”?"
- "ì„¤ë¬¸ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ í”¼ë¶€ ì‚¬ì§„ì„ ì´¬ì˜í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì„œ ë” ì •í™•í•œ ë¶„ì„ì„ ë°›ì•„ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤ ğŸ“¸âœ¨"

# ì˜ˆì‹œ

ì˜ˆì‹œ 1 (ëŒ€í™” ì´ˆë°˜, ì—°ë ¹ëŒ€/í”¼ë¶€ ê³ ë¯¼ ìˆ˜ì§‘):

"ì•ˆë…•í•˜ì„¸ìš”! í”¼ë¶€ ìƒë‹´ ì±—ë´‡ì´ì—ìš”ğŸ˜Š ì˜¤ëŠ˜ í”¼ë¶€ì— ëŒ€í•´ ì–´ë–¤ ê³ ë¯¼ì„ ê°–ê³  ê³„ì‹ ê°€ìš”?  
ì—°ë ¹ëŒ€ë„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”!  
1) 10ëŒ€  2) 20ëŒ€  3) 30ëŒ€  4) 40ëŒ€ ì´ìƒ  
ì£¼ìš” í”¼ë¶€ ê³ ë¯¼ë„ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”."

ì˜ˆì‹œ 2 (ë‹µë³€ì— ê³µê° í›„ ë‹¤ìŒ ë‹¨ê³„ë¡œ):

"20ëŒ€ì‹œêµ°ìš”, ë§ì”€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! í˜¹ì‹œ ì´ì „ì— í”¼ë¶€ íŠ¸ëŸ¬ë¸”(ì˜ˆ: ì—¬ë“œë¦„, ì•Œë ˆë¥´ê¸°) ê²½í—˜ì´ ìˆìœ¼ì…¨ë‚˜ìš”?  
ìˆë‹¤ë©´ ì–´ë–¤ íŠ¸ëŸ¬ë¸”ì´ì—ˆëŠ”ì§€, ì—†ìœ¼ì‹œë©´ 'ì—†ìŒ'ì´ë¼ê³  ë‹µí•´ì£¼ì…”ë„ ê´œì°®ì•„ìš”ğŸ˜Š"

ì˜ˆì‹œ 3 (ë§ˆì§€ë§‰ ìš”ì•½):

"ì§€ê¸ˆê¹Œì§€ ì•Œë ¤ì£¼ì‹  ì •ë³´ë¥¼ ì •ë¦¬í•´ë³´ë©´,  
- ì—°ë ¹ëŒ€: 20ëŒ€  
- ì£¼ìš” ê³ ë¯¼: ê±´ì¡°í•¨  
- ê³¼ê±° íŠ¸ëŸ¬ë¸”: ì—†ìŒ  
- í˜„ì¬ ë£¨í‹´: í´ë Œì €, í† ë„ˆë§Œ ì‚¬ìš©  
ì´ë ‡ê²Œ ì •ë¦¬í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. í˜¹ì‹œ ì¶”ê°€ë¡œ ë§ì”€í•´ì£¼ì‹¤ ë‚´ìš©ì´ë‚˜ ìˆ˜ì •í•  ì ì´ ìˆìœ¼ì‹¤ê¹Œìš”?

ì„¤ë¬¸ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ í”¼ë¶€ ì‚¬ì§„ì„ ì´¬ì˜í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì„œ ë” ì •í™•í•œ ë¶„ì„ì„ ë°›ì•„ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤ ğŸ“¸âœ¨"

(ì‹¤ì œ ëŒ€í™”ì—ì„œëŠ” ê° ë‹¨ê³„ì— ë§ëŠ” ë” ìƒì„¸í•œ ë‹µë³€ê³¼ ë‹¤ì–‘í•œ ê³ ë¯¼, ìŠµê´€ ë“±ì— ëŒ€í•œ ë‚´ìš©ì´ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

---

*ì¤‘ìš”ì‚¬í•­ ìš”ì•½:  
- í•­ìƒ ê³µê°ì ìœ¼ë¡œ, 1~2ê°œ ì§ˆë¬¸ê³¼ ì„ íƒì§€ ì¤‘ì‹¬  
- ê°œì¸ì •ë³´Â·ì§„ë‹¨ NO, ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°  
- ë§ˆì§€ë§‰ì—” ë‹µë³€ ìš”ì•½ í›„ í™•ì¸ ì§ˆë¬¸*
"""

# LangChain/LangGraph í†µí•© ì„í¬íŠ¸
try:
    from langchain_integration import analyze_skin_with_langchain
    LANGCHAIN_AVAILABLE = True
    logger.info("âœ… LangChain/LangGraph í†µí•© ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    logger.warning(f"âš ï¸ LangChain/LangGraph ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

# LangChain ê¸°ë°˜ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze-skin-langchain")
async def analyze_skin_langchain(request: dict):
    """LangChain/LangGraph ê¸°ë°˜ ë§ˆìŠ¤í„°-ìŠ¬ë ˆì´ë¸Œ í”¼ë¶€ ë¶„ì„"""
    global analyzer
    
    if not LANGCHAIN_AVAILABLE:
        # LangChainì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ê¸° ì‚¬ìš© (í˜¸í™˜ì„± ìœ ì§€)
        logger.warning("âš ï¸ LangChain ëª¨ë“ˆì´ ì—†ì–´ ê¸°ë³¸ ë¶„ì„ê¸° ì‚¬ìš©")
        
        if analyzer is None:
            raise HTTPException(status_code=503, detail="AI ë¶„ì„ê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            image_data = request.get('image')
            chatbot_data = request.get('chatbot_data')
            
            if not image_data:
                raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # Base64 ë””ì½”ë”©
            image_data_clean = image_data.split(',')[1] if ',' in image_data else image_data
            image_bytes = base64.b64decode(image_data_clean)
            nparr = np.frombuffer(image_bytes, np.uint8)
            image_array = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
            
            # ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰ (ë” ë¯¼ê°í•œ ì„¤ì •)
            result = await analyzer.analyze_image_advanced(image_array, chatbot_data=chatbot_data)
            result_dict = asdict(result)
            
            return {
                "success": True,
                "analysis_method": "enhanced_fallback_analysis",
                "result": result_dict,
                "agents_used": [
                    "Enhanced OpenCV Face Detection",
                    "Sensitive Acne Detection", 
                    "Improved Coordinate Mapping",
                    "OpenAI API Integration"
                ]
            }
            
        except Exception as e:
            logger.error(f"Fallback ë¶„ì„ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    try:
        image_data = request.get('image')
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        
        if not image_data:
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        logger.info("ğŸš€ LangChain/LangGraph ë§ˆìŠ¤í„°-ìŠ¬ë ˆì´ë¸Œ ë¶„ì„ ì‹œì‘...")
        
        # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await analyze_skin_with_langchain(image_data, openai_api_key)
        
        logger.info("âœ… LangChain/LangGraph ë¶„ì„ ì™„ë£Œ!")
        
        return {
            "success": True,
            "analysis_method": "langchain_langraph_workflow",
            "processing_time": "multi_agent_parallel",
            "result": result,
            "agents_used": [
                "MediaPipe Master",
                "Face-API.js Slave", 
                "Nyckel API Slave",
                "OpenCV Preprocessing Slave"
            ]
        }
        
    except Exception as e:
        logger.error(f"LangChain ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"LangChain ê¸°ë°˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ì±—ë´‡ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/chat")
async def chat_consultation(request: ChatRequest) -> ChatResponse:
    """
    í”¼ë¶€ ìƒë‹´ ì±—ë´‡ API
    """
    try:
        if not openai_client:
            raise HTTPException(
                status_code=503,
                detail="OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [
            {
                "role": "system",
                "content": SKIN_CONSULTATION_PROMPT
            }
        ]
        
        # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({
            "role": "user",
            "content": f"[ë‹¨ê³„ {request.user_stage}] {request.message}"
        })
        
        # OpenAI GPT-4o API í˜¸ì¶œ
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=500,
            temperature=0.7,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0
        )
        
        bot_response = response.choices[0].message.content
        
        # ë‹¨ê³„ ì§„í–‰ ë¡œì§
        next_stage = request.user_stage
        is_final = False
        
        # ê°„ë‹¨í•œ ë‹¨ê³„ ì§„í–‰ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)
        if "ìš”ì•½" in bot_response or "ì •ë¦¬í•´ë³´ë©´" in bot_response:
            is_final = True
        elif request.user_stage < 6:
            next_stage = request.user_stage + 1
        
        # ìˆ˜ì§‘ëœ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì •ë³´ ì¶”ì¶œ ë¡œì§ í•„ìš”)
        collected_info = None
        if is_final and request.conversation_history:
            collected_info = {
                "conversation_summary": "ì‚¬ìš©ì ìƒë‹´ ì™„ë£Œ",
                "total_messages": len(request.conversation_history) + 1
            }
        
        return ChatResponse(
            message=bot_response,
            stage=next_stage,
            is_final=is_final,
            collected_info=collected_info
        )
        
    except Exception as e:
        logger.error(f"ì±—ë´‡ API ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì±—ë´‡ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@app.get("/api/chat/status")
async def get_chat_status():
    """
    ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    """
    return {
        "chatbot_available": openai_client is not None,
        "model": "gpt-4o",
        "status": "active" if openai_client else "inactive"
    }

@app.get("/proxy-image")
async def proxy_image(url: str):
    """
    í™”í•´ ì´ë¯¸ì§€ CORS í”„ë¡ì‹œ ì„œë²„
    """
    try:
        from fastapi.responses import StreamingResponse
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://www.hwahae.co.kr/',
            'Connection': 'keep-alive',
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    content = await response.read()
                    content_type = response.headers.get('Content-Type', 'image/jpeg')
                    
                    return StreamingResponse(
                        io.BytesIO(content), 
                        media_type=content_type,
                        headers={
                            'Cache-Control': 'public, max-age=3600',
                            'Access-Control-Allow-Origin': '*'
                        }
                    )
                else:
                    raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ í”„ë¡ì‹œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)